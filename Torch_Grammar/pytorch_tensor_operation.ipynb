{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "gpu_device: cpu\n",
      "cuda deviceGeForce RTX 2060: cuda\n",
      "tensor([[0.3536, 0.7971, 0.3799],\n",
      "        [0.2110, 0.3212, 0.3842]])\n",
      "tensor([[0.3578, 0.0067, 0.1944],\n",
      "        [0.9394, 0.2579, 0.3824]], device='cuda:0')\n",
      "tensor([[0.8969, 0.7925, 0.2236],\n",
      "        [0.7228, 0.7313, 0.3151]], device='cuda:0')\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[1.3578, 1.0067, 1.1944],\n",
      "        [1.9394, 1.2579, 1.3824]], device='cuda:0')\n",
      "tensor([[1.3578, 1.0067, 1.1944],\n",
      "        [1.9394, 1.2579, 1.3824]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# check pytorch gpu is avalible\n",
    "\n",
    "import torch\n",
    "\n",
    "# version\n",
    "print(torch.__version__)\n",
    "\n",
    "# check cpu and gpu device\n",
    "cpu_device = torch.device('cpu')\n",
    "cuda_device = torch.device('cuda')\n",
    "\n",
    "print('gpu_device:', cpu_device)\n",
    "print('cuda device{0}:'.format(torch.cuda.get_device_name(0)),  cuda_device)\n",
    "\n",
    "# transform tensor between cpu and gpu\n",
    "if torch.cuda.is_available():\n",
    "    # get gpu device\n",
    "    cuda_device = torch.device('cuda')\n",
    "    \n",
    "    # default device is cpu\n",
    "    # create tensor on cpu\n",
    "    x_0 = torch.rand(2, 3)\n",
    "    # create tensor on cuda method 1\n",
    "    x_1 = torch.rand(2, 3).cuda()\n",
    "    # create tensor on cuda method 2\n",
    "    x_2 = torch.rand((2 ,3), device=cuda_device)\n",
    "    \n",
    "    y_0 = torch.ones_like(x_1, device=cuda_device)\n",
    "    \n",
    "    # exexute plus in gpu\n",
    "    z_0 = x_1 + y_0\n",
    "    # transform tensor to cpu\n",
    "    z_1 = z_0.to('cpu', dtype=torch.double)\n",
    "    \n",
    "    print(x_0)\n",
    "    print(x_1)\n",
    "    print(x_2)\n",
    "    print(y_0)\n",
    "    print(z_0) \n",
    "    print(z_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0393e+34,  4.5712e-41, -2.0393e+34],\n",
      "        [ 4.5712e-41,  4.4842e-44,  0.0000e+00],\n",
      "        [ 1.1210e-43,  0.0000e+00, -3.5687e+05],\n",
      "        [ 3.0826e-41, -2.0393e+34,  4.5712e-41],\n",
      "        [-3.2495e-09,  0.0000e+00,  1.8077e-43]])\n",
      "tensor([[0.4588, 0.0725, 0.3630],\n",
      "        [0.9648, 0.6227, 0.7576],\n",
      "        [0.7334, 0.0565, 0.6274],\n",
      "        [0.0550, 0.6478, 0.9163],\n",
      "        [0.0010, 0.6747, 0.3833]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([2., 3.])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.3932, -1.7211, -0.3417,  0.9892,  1.0600],\n",
      "        [-2.4091, -0.7594, -1.5943,  0.8354, -1.6701],\n",
      "        [-1.8651,  0.3931,  1.7805,  0.7994,  0.7682]])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# create tensor\n",
    "import torch\n",
    "\n",
    "x_1 = torch.empty(5, 3)\n",
    "print(x_1)\n",
    "\n",
    "x_2 = torch.rand(5, 3)\n",
    "print(x_2)\n",
    "\n",
    "x_3 = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x_3)\n",
    "\n",
    "x_4 = torch.tensor([2.0, 3])\n",
    "print(x_4)\n",
    "\n",
    "\n",
    "# reuse tensor property\n",
    "x_5 = x_2.new_ones(3, 5, dtype=torch.float64)\n",
    "print(x_5)\n",
    "\n",
    "# assign new type\n",
    "x_6 = torch.randn_like(x_5, dtype=torch.float)\n",
    "print(x_6)\n",
    "\n",
    "# print tensor shape\n",
    "print(x_6.size())\n",
    "print(x_6.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0148, 1.4659, 0.8885, 0.2931, 0.5897],\n",
      "        [1.3157, 1.4053, 1.2935, 1.3298, 1.0517],\n",
      "        [0.9018, 1.3545, 0.0585, 0.3541, 0.6673]])\n",
      "tensor([[1.0148, 1.4659, 0.8885, 0.2931, 0.5897],\n",
      "        [1.3157, 1.4053, 1.2935, 1.3298, 1.0517],\n",
      "        [0.9018, 1.3545, 0.0585, 0.3541, 0.6673]])\n",
      "tensor([[1.0148, 1.4659, 0.8885, 0.2931, 0.5897],\n",
      "        [1.3157, 1.4053, 1.2935, 1.3298, 1.0517],\n",
      "        [0.9018, 1.3545, 0.0585, 0.3541, 0.6673]])\n",
      "tensor([1.4963, 1.7682, 1.0885, 1.1320, 1.3074])\n",
      "tensor([1.4963, 1.7682, 1.0885, 1.1320, 1.3074])\n",
      "tensor([[0.6341, 0.4901, 0.8964, 0.4556, 0.6323],\n",
      "        [0.3489, 0.4017, 0.0223, 0.1689, 0.2939]])\n",
      "tensor([[1.4963, 1.4963, 1.4963],\n",
      "        [0.8964, 0.8964, 0.6323],\n",
      "        [0.4017, 0.4017, 0.1689]])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([15])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([3, 5])\n",
      "tensor([2.4963, 2.7682, 2.0885, 2.1320, 2.3074, 1.6341, 1.4901, 1.8964, 1.4556,\n",
      "        1.6323, 1.3489, 1.4017, 1.0223, 1.1689, 1.2939])\n",
      "tensor([[2.4963, 2.7682, 2.0885, 2.1320, 2.3074],\n",
      "        [1.6341, 1.4901, 1.8964, 1.4556, 1.6323],\n",
      "        [1.3489, 1.4017, 1.0223, 1.1689, 1.2939]])\n",
      "tensor([3.4963, 3.7682, 3.0885, 3.1320, 3.3074, 2.6341, 2.4901, 2.8964, 2.4556,\n",
      "        2.6323, 2.3489, 2.4017, 2.0223, 2.1689, 2.2939])\n",
      "tensor([[2.4963, 2.7682, 2.0885, 2.1320, 2.3074],\n",
      "        [1.6341, 1.4901, 1.8964, 1.4556, 1.6323],\n",
      "        [1.3489, 1.4017, 1.0223, 1.1689, 1.2939]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# add operation\n",
    "x = torch.rand(3, 5)\n",
    "y = torch.rand(3, 5)\n",
    "\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# allocate output\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "# inplace\n",
    "y.add_(x)\n",
    "print(y)\n",
    "\n",
    "\n",
    "# indice\n",
    "# note 索引出来的结果和原数据共享内存，修改索引后数据，原数据也会被修改\n",
    "y = x[0, :]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :])\n",
    "\n",
    "# index_select\n",
    "y = torch.index_select(x, dim=0, index=torch.tensor([1, 2]))\n",
    "print(y)\n",
    "\n",
    "# 根据维度获取聚合值\n",
    "y = torch.gather(x, dim=1, index=torch.LongTensor([[0, 0, 0], [2, 2, 4], [1, 1, 3]]))\n",
    "print(y)\n",
    "\n",
    "\n",
    "# change shape\n",
    "print(x.shape)\n",
    "y = x.view(torch.mul(x.shape[0], x.shape[1]))\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "z = x.view(-1, 3)\n",
    "print(z.shape)\n",
    "print(x.shape)\n",
    "\n",
    "# view 返回的tensor虽然可能与原tensor的shape不同， 但是仍然共享data\n",
    "# view 仅仅是改变了对这个张量的观察角度，内部数据并未改变\n",
    "y += 1\n",
    "print(y)\n",
    "print(x) # x change follwed y\n",
    "\n",
    "# use deep copy\n",
    "# clone 会被记录在计算图中， 即梯度回传到副本时也会传到源tensor\n",
    "y_cp  = x.clone().view(torch.mul(x.shape[0], x.shape[1])) # 对副本做view\n",
    "y_cp += 1\n",
    "print(y_cp)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9528])\n",
      "-0.9528351426124573\n",
      "[-0.95283514]\n",
      "tensor([[1, 2],\n",
      "        [2, 3],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 获取标量（scalar）的元素值\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "\n",
    "# tensor 转换为 numpy\n",
    "# array convert to numpy\n",
    "y = x.clone().numpy()\n",
    "print(y)\n",
    "\n",
    "\n",
    "# broadcasting\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(0, 3).view(3, 1)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# memery cost\n",
    "# 索引不开辟内存，运算操作会开辟新的内存\n",
    "# view 返回的tensor与源tensor共享data, 但是依然是一个新的tensor, 二者id(内存地地址并不一致)\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = x + y\n",
    "print(id(y) == id_before)\n",
    "\n",
    "# 利用索引替换操作\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = x + y\n",
    "print(id(y) == id_before)\n",
    "\n",
    "# 利用运算符全名函数中out参数\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y)\n",
    "print(id(y) == id_before)\n",
    "\n",
    "# 利用自加运算符\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y += x\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n",
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "[4. 4. 4. 4. 4.] tensor([4., 4., 4., 4., 4.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy 与 tensor 的相互转换\n",
    "import torch\n",
    "import numpy as np\n",
    "# tensor => numpy\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "# numpy() 所产生的数组和tensor共享相同和内存\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n",
    "\n",
    "# numpy => tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "# from_numpy() 所产生的tensor和数组共享相同和内存\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n",
    "\n",
    "# torch.tensor 会对数据进行拷贝(copy), 返回的tensor 与原来的数据不再共享内存\n",
    "c = torch.tensor(a)\n",
    "a +=1\n",
    "print(a, c)\n",
    "c += 1\n",
    "print(a, c)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
