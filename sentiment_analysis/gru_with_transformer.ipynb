{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer with bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "from torchtext import data\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random config\n",
    "SEED = 2021\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load transformer pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize test\n",
    "\n",
    "tokenize and lower case the data in a way that is consistent with the pre-trained transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'are', 'you', 'ok', '?']\n"
     ]
    }
   ],
   "source": [
    "demo_text = 'Hello WORLD ARE yoU OK?'\n",
    "tokens = tokenizer.tokenize(demo_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  numericalize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2024, 2017, 7929, 1029]\n"
     ]
    }
   ],
   "source": [
    "token_indices = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the tokenizer does have a beginning of sequence and end of sequence attributes (bos_token and eos_token) but these are not set and should not be used for this transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "bos_token = tokenizer.cls_token # classifier token which is used when doing sequence classification\n",
    "eos_token = tokenizer.sep_token # The separator token, which is used when building a sequence from multiple sequences\n",
    "pad_token = tokenizer.pad_token # The token used for padding\n",
    "unk_token = tokenizer.unk_token # The unknown token\n",
    "\n",
    "print(bos_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the indices of the special tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n",
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "# by converting them using the vocabulary\n",
    "bos_token_idx = tokenizer.convert_tokens_to_ids(bos_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "print(bos_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
    "\n",
    "# explicit getting inddex with from tokenizer\n",
    "bos_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "print(bos_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defined maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer and cut\n",
    "\n",
    "*Note* that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/alex/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_with_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = bos_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Dataset.__getattr__ at 0x7feb6d6a37d8>\n",
      "<torchtext.data.example.Example object at 0x7feb6cfc00f0>\n",
      "Number of train data 17500\n",
      "Number of val data 7500\n",
      "Number of val data 25000\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "# -----------------get train, val and test data--------------------\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root='../Dataset/IMDB')\n",
    "\n",
    "print(train_data.fileds)\n",
    "print(train_data.examples[0])\n",
    "\n",
    "train_data, val_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print('Number of train data {}'.format(len(train_data)))\n",
    "print('Number of val data {}'.format(len(val_data)))\n",
    "print('Number of val data {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check an example and ensure that the text has already been numericalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [2025, 2172, 2000, 2360, 2006, 2023, 2028, 1012, 1037, 5436, 2017, 2064, 3492, 2172, 25039, 1010, 1999, 1996, 2034, 2184, 2781, 1012, 2498, 15241, 3308, 2007, 2023, 2143, 1010, 2200, 2210, 2895, 2005, 2019, 2895, 2143, 1012, 2045, 2001, 1037, 3382, 2000, 8849, 1996, 3494, 6699, 5681, 1012, 3251, 2019, 2895, 2143, 2003, 1996, 2157, 6907, 2000, 2079, 2008, 2007, 1010, 1045, 1005, 1049, 2145, 6151, 8586, 14097, 1012, 17515, 2001, 2028, 1997, 1996, 25551, 3152, 2000, 3422, 2302, 3228, 2440, 3086, 2000, 1010, 2004, 2009, 2018, 2210, 21438, 1998, 1037, 19647, 5436, 1012, 1045, 2001, 2763, 5905, 1997, 2008, 1010, 2061, 2007, 1037, 2117, 3422, 2030, 2007, 6151, 12848, 14097, 3086, 2009, 2089, 2022, 2488, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1018, 1013, 2184, 1006, 2021, 1996, 2190, 1997, 2026, 1018, 2041, 1997, 2184, 1005, 1055, 1007], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " transform these indexes back into readable tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eddie', 'murphy', 'really', 'made', 'me', 'laugh', 'my', 'ass', 'off', 'on', 'this', 'hbo', 'stand', 'up', 'comedy', 'show', '.', 'i', 'love', 'his', 'impressions', 'of', 'mr', '.', 't', ',', 'ed', 'norton', 'and', 'ralph', 'cr', '##am', '##den', 'of', '\"', 'the', 'honeymoon', '##ers', '\"', ',', 'elvis', 'presley', ',', 'and', 'michael', 'jackson', 'too', '.', 'the', 'ice', 'cream', 'man', ',', 'goo', '##ny', 'goo', 'goo', ',', 'is', 'also', 'funny', '.', 'i', 'saw', 'this', 'for', 'the', 'first', 'time', 'when', 'it', 'came', 'out', 'in', '1984', '.', 'i', 'laughed', 'so', 'hard', ',', 'i', 'almost', 'fell', 'off', 'my', 'chair', '.', 'i', 'still', 'think', 'this', 'is', 'very', 'funny', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'eddie', 'murphy', ',', 'when', 'he', 'was', 'on', '\"', 'saturday', 'night', 'live', '\"', ',', 'made', 'me', 'laugh', 'so', 'hard', ',', 'he', 'is', 'one', 'of', 'the', 'best', 'people', 'to', 'come', 'out', 'of', '\"', 'saturday', 'night', 'live', '\"', '.', '\"', 'eddie', 'murphy', 'del', '##iri', '##ous', '\"', 'is', 'his', 'best', 'stand', 'up', 'performance', 'next', 'to', '\"', 'eddie', 'murphy', 'raw', '\"', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'give', '\"', 'eddie', 'murphy', 'del', '##iri', '##ous', '\"', '2', 'thumbs', 'up', 'and', '10', '/', '10', 'stars', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build the vocabulary for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
