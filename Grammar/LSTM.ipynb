{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于LSTM的字符生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "长短期记忆（long short-term memory，LSTM）是一种常用的门控循环神经网络。\n",
    "\n",
    "LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞，从而记录额外的信息。\n",
    "\n",
    "![LSTM](../Docs/LSTM-Core.png)\n",
    "\n",
    "<center>http://dprogrammer.org/rnn-lstm-gru</center>\n",
    "\n",
    "* 输入门、遗忘门、输出门计算\n",
    "\n",
    "$$ \\begin{aligned} \n",
    "\\boldsymbol{I}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xi} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hi} + \\boldsymbol{b}_i),\\\\ \n",
    "\\boldsymbol{F}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xf} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hf} + \\boldsymbol{b}f),\\\\ \n",
    "\\boldsymbol{O}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xo} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o), \\end{aligned} $$\n",
    "\n",
    "* 候选记忆细胞计算\n",
    "\n",
    "$$ \\tilde{\\boldsymbol{C}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xc} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hc} + \\boldsymbol{b}_c), $$\n",
    "\n",
    "* 更新记忆细胞\n",
    "$$\\boldsymbol{C}_t = \\boldsymbol{F}_t \\odot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_t \\odot \\tilde{\\boldsymbol{C}}_t.$$\n",
    "\n",
    "* 更新隐藏状态\n",
    "\n",
    "$$\\boldsymbol{H}_t = \\boldsymbol{O}_t \\odot \\text{tanh}(\\boldsymbol{C}_t).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import zipfile\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import utils as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(object):\n",
    "    def __init__(self, text=None, max_vocab=None, filename=None):\n",
    "        if filename is not None:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.vocab = pickle.load(f)\n",
    "        else:\n",
    "            corpus_chars = set(text)\n",
    "            # max_vocab_process\n",
    "            vocab_count = {}\n",
    "            for word in corpus_chars:\n",
    "                if vocab_count.get(word) is None:\n",
    "                    vocab_count[word] = 1\n",
    "                else:\n",
    "                    vocab_count[word] += 1\n",
    "\n",
    "            vocab_count_list = []\n",
    "            for word in vocab_count:\n",
    "                vocab_count_list.append((word, vocab_count[word]))\n",
    "            # sort count with number\n",
    "            vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            if max_vocab is not None and len(vocab_count_list) > max_vocab:\n",
    "                vocab_count_list = vocab_count_list[:max_vocab]\n",
    "            vocab = [x[0] for x in vocab_count_list]\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.char_to_index = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.index_to_char = dict(enumerate(self.vocab))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        if word in self.char_to_index:\n",
    "            return self.char_to_index[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def index_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.char_to_index[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_array(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_index(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def array_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.index_to_word(index))\n",
    "        return \"\".join(words)\n",
    "\n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample\n",
    "def data_loader_random(corpus_indices, seq_length, time_steps, device=None):\n",
    "    num_samples = (len(corpus_indices) - 1) // time_steps\n",
    "    epoch_size = num_samples // seq_length\n",
    "\n",
    "    # shuffle sample\n",
    "    samples_indices = list(range(num_samples))\n",
    "    random.shuffle(samples_indices)\n",
    "\n",
    "    # generator data\n",
    "    for i in range(epoch_size):\n",
    "        i = i * seq_length\n",
    "\n",
    "        batch_incices = samples_indices[i: i + seq_length]\n",
    "        x = [corpus_indices[indices: indices + time_steps] for indices in batch_incices]\n",
    "        y = [corpus_indices[indices + 1: indices + time_steps + 1] for indices in batch_incices]\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32).view(seq_length, time_steps)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(seq_length, time_steps)\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consecutive sample\n",
    "def data_loader_consecutive(corpus_indices, seq_length, time_steps):\n",
    "\n",
    "    data_len = len(corpus_indices)\n",
    "\n",
    "    seq_size = data_len // seq_length\n",
    "\n",
    "    # resize to => (seq_length, seq_size)\n",
    "    corpus_indices = np.array(corpus_indices[: seq_size * seq_length], dtype=np.float).reshape((seq_length, -1))\n",
    "\n",
    "    epoch_size = (seq_size - 1) // time_steps\n",
    "\n",
    "    # generator data\n",
    "    np.random.shuffle(corpus_indices)\n",
    "\n",
    "    # convert to torch tensor\n",
    "    torch_indices = torch.tensor(corpus_indices, dtype=torch.float32).view(seq_length, seq_size)\n",
    "    for i in range(epoch_size):\n",
    "        i = i * time_steps\n",
    "        x = torch_indices[:, i: i + time_steps]\n",
    "        y = torch_indices[:, i + 1: i + time_steps + 1]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../Datasets/jaychou_lyrics/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')  # corpus\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\t', ' ')\n",
    "corpus_chars = corpus_chars[: 20000]\n",
    "\n",
    "tokenizer = TextTokenizer(text=corpus_chars, max_vocab=None)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "char_to_index = tokenizer.char_to_index\n",
    "index_to_char = tokenizer.index_to_char\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "corpus_indices = tokenizer.text_to_array(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1448])\n"
     ]
    }
   ],
   "source": [
    "def onehot(x, n_class):\n",
    "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n",
    "\n",
    "    return [F.one_hot(x[:, i].to(torch.int64), n_class).to(dtype=torch.float32, device=device) for i in range(x.shape[1])]\n",
    "\n",
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(seq_length, num_hiddens, device):\n",
    "    return (torch.zeros((seq_length, num_hiddens), device=device),\n",
    "            torch.zeros((seq_length, num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "input_size, hidden_size, output_size = vocab_size, 256, vocab_size\n",
    "# initial model params\n",
    "def get_params():\n",
    "    \n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    def _three():\n",
    "        return (_one((input_size, hidden_size)),\n",
    "                _one((hidden_size, hidden_size)),\n",
    "                nn.Parameter(torch.zeros(hidden_size, device=device, dtype=torch.float32),requires_grad=True))\n",
    "    \n",
    "    # hidden params\n",
    "    w_xi, w_hi, b_i = _three() # input gate\n",
    "    w_xf, w_hf, b_f = _three() # forget gate\n",
    "    w_xo, w_ho, b_o = _three() # output gate\n",
    "\n",
    "    w_xc, w_hc, b_c = _three()  # candidate memory \n",
    "    \n",
    "    # output params\n",
    "    w_hq = _one((hidden_size, output_size))\n",
    "    b_q = nn.Parameter(torch.zeros(output_size, device=device, dtype=torch.float32), requires_grad=True)\n",
    "    \n",
    "    return nn.ParameterList([w_xi, w_hi, b_i, w_xf, w_hf, b_f, w_xo, w_ho, b_o, w_xc, w_hc, b_c, w_hq, b_q])\n",
    "\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print('will use', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "def lstm(inputs, states, params):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs: (time_steps, seq_len, vocab_size)\n",
    "    :param states:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    (h, c) = states\n",
    "    w_xi, w_hi, b_i, w_xf, w_hf, b_f, w_xo, w_ho, b_o, w_xc, w_hc, b_c, w_hq, b_q = params\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for input in inputs:\n",
    "        i_t = torch.sigmoid(torch.matmul(input, w_xi) + torch.matmul(h, w_hi) + b_i)\n",
    "        f_t = torch.sigmoid(torch.matmul(input, w_xf) + torch.matmul(h, w_hf) + b_f)\n",
    "        o_t = torch.sigmoid(torch.matmul(input, w_xo) + torch.matmul(h, w_ho) + b_o)\n",
    "\n",
    "        c_tilde = torch.tanh(torch.matmul(input, w_xc) + torch.matmul(h, w_hc) + b_c)\n",
    "\n",
    "        c = torch.mul(f_t, c) + torch.mul(i_t, c_tilde) # update carry state\n",
    "        \n",
    "        h = torch.mul(o_t, c.tanh()) # update hidden state\n",
    "        \n",
    "        y = torch.matmul(h, w_hq) + b_q\n",
    "      \n",
    "        outputs.append(y)\n",
    "        \n",
    "    return outputs, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1448])\n",
      "5 torch.Size([2, 1448]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(10).view(2, 5)\n",
    "state = init_lstm_state(X.shape[0], hidden_size, device)\n",
    "inputs = onehot(X.to(device), vocab_size)\n",
    "print(inputs[0].size())\n",
    "params = get_params()\n",
    "outputs, state_new = lstm(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(prefix, num_chars, lstm, params, init_lstm_state, \n",
    "                 num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n",
    "    state = init_lstm_state(1, num_hiddens, device)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        \n",
    "        # previous last outputs as current input\n",
    "        x = onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n",
    "        # comuper outputs and state\n",
    "        (y, state) = lstm(x, state, params)\n",
    "        \n",
    "        # convert output from index to char\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好想军椅觉静忍威持占植蔓\n"
     ]
    }
   ],
   "source": [
    "p = predict_lstm('好想', 10, lstm, params, init_lstm_state, hidden_size, vocab_size,\n",
    "                    device, index_to_char, char_to_index)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_lstm(lstm, get_params, init_lstm_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, seq_length, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = data_loader_random\n",
    "    else:\n",
    "        data_iter_fn = data_loader_consecutive\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # \n",
    "            state = init_lstm_state(seq_length, num_hiddens, device)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, seq_length, num_steps)\n",
    "        for X, Y in data_iter:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            if is_random_iter:  \n",
    "                state = init_lstm_state(seq_length, num_hiddens, device)\n",
    "            else:\n",
    "                #\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "\n",
    "            inputs = onehot(X, vocab_size)\n",
    "        \n",
    "            (outputs, state) = lstm(inputs, state, params)\n",
    "        \n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "        \n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "            # cross entropy\n",
    "            l = loss(outputs, y.long())\n",
    "\n",
    "            # gradient clearing\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            # gradient clearing\n",
    "            grad_clipping(params, clipping_theta, device) \n",
    "            \n",
    "            d2l.sgd(params, lr, 1)\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_lstm(prefix, pred_len, lstm, params, init_lstm_state,\n",
    "                                        num_hiddens, vocab_size, device, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, time_steps, seq_length, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['好想', '不想']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 1.070160, time 0.60 sec\n",
      " - 好想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的\n",
      " - 不想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      "epoch 100, perplexity 1.057306, time 0.58 sec\n",
      " - 好想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      " - 不想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      "epoch 150, perplexity 1.054995, time 0.57 sec\n",
      " - 好想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      " - 不想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      "epoch 200, perplexity 1.053482, time 0.59 sec\n",
      " - 好想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      " - 不想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔\n",
      "epoch 250, perplexity 1.053013, time 0.58 sec\n",
      " - 好想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让\n",
      " - 不想想著你 这样 甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让\n"
     ]
    }
   ],
   "source": [
    "# random sample\n",
    "random_sample = True\n",
    "train_and_predict_lstm(lstm, get_params, init_lstm_state, hidden_size,\n",
    "                      vocab_size, device, corpus_indices, index_to_char,\n",
    "                      char_to_index, random_sample, num_epochs, time_steps, lr,\n",
    "                      clipping_theta, seq_length, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 109.663761, time 0.57 sec\n",
      " - 好想 我不要你不要 你不要我想要 我不要我不要 你不不再爱你 我不要我不要 你不不再爱你 我不要我不要 \n",
      " - 不想 我不要你不要 你不要我想要 我不要我不要 你不不再爱你 我不要我不要 你不不再爱你 我不要我不要 \n",
      "epoch 100, perplexity 11.054566, time 0.57 sec\n",
      " - 好想 我不想这想 你不是陪不不开 我不要这样你 你永是我不要 你永远 你不是我不要 你不去 你不么我不多\n",
      " - 不想 你不么这不打 我不了这样活 我该会不生 你的手不是 你不了好离 我永远 你走了离开 我知好好生活 \n",
      "epoch 150, perplexity 2.629018, time 0.57 sec\n",
      " - 好想要你 我要不想 我不不再 我不能 我不我 不想这样 你不么 你不开不开 不知不觉 你跟经这节奏 后知\n",
      " - 不想要你的手 想要到你去去 我只到到你已离开 我不要这样己  说是你不要 不要帮你不多说不要我带不难 我\n",
      "epoch 200, perplexity 1.417030, time 0.62 sec\n",
      " - 好想要多场的剧 从想连无都都有不去 坐馨的黑乐喜上 周杰来的旧 这一种味道 做人的美丽 是风完著日 这里\n",
      " - 不想 这你为么简你 一直忙的话作 我的天都都的你 想想了直去的勇 我想着打慢的想脸 我不揍你已经很久 别\n",
      "epoch 250, perplexity 1.167032, time 0.57 sec\n",
      " - 好想要一直场 我想你 这手对听 没有它着地暗乐下愿 看必让让点里方祷记 一直一阵以你 你经忆美不到 我想\n",
      " - 不想要 我只想想你的我 不要再觉汉活 是什么我说分开 我想 我只会陪下着你手不放你 爱可可可以简单单没没\n"
     ]
    }
   ],
   "source": [
    "# consecutive sample\n",
    "random_sample = False\n",
    "train_and_predict_lstm(lstm, get_params, init_lstm_state, hidden_size,\n",
    "                      vocab_size, device, corpus_indices, index_to_char,\n",
    "                      char_to_index, random_sample, num_epochs, time_steps, lr,\n",
    "                      clipping_theta, seq_length, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
