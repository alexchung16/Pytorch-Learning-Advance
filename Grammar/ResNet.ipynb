{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 残差网络\n",
    "\n",
    "构建深度神经网络时会遇到两个主要问题：梯度爆炸/消散和网络退化问题。\n",
    "* 梯度爆炸/消散，是由于随着层数的增加，在网络反向传播过程中梯度会随着级乘运算变得特别大或特别小，即梯度变得不稳定。可以通过BatchNrom技术解决。\n",
    "* 网络退化，通常认为，随着网络深度的增加，网络的性能会获得相应的提升。**但是，我们发现当网络增加到一定程度后继续增加，网络的性能会变得越來越差，直接体现为训练集上的准确率会下降。** 我们假设通过简单地对叠方式得到很深的网络，网络内部的特征在某一层（浅层）已经达到最佳的性能，此时该网络的浅层形式的解空间是深层模型解空间的子空间。也就是说，如果我们能够将达到最佳性能的层之后的层训练成恒等映射，且深层网络可能得出更优的解来拟合训练集，因此深层网络能够更容易地降低训练误差。**但是，由于网络退化的问题，这一假设并不成立。**\n",
    "* 通过分析，我们退而求其次，在已知深层网络存在退化的的情况下，寻求方法解决深层网络的退化问题，使得网络至少实现深层网络和浅层网络具有一样的性能。即让深层网络后面的部分即使不提升网络性能的情况下，至少能够实现恒等映射的作用，使得网络的性能不会随着深度的增加而出现退化。Residual模块被提出来解决这一问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, size=None, num_workers=4):\n",
    "    # dataset process\n",
    "    trans = []\n",
    "    if size:\n",
    "        trans.append(torchvision.transforms.Resize(size=size))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "\n",
    "    # load\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=True, download=True,\n",
    "                                                    transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=False, download=True,\n",
    "                                                   transform=transform)\n",
    "    # generate\n",
    "    train_generator = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_generator = data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "### residual block\n",
    "![ResNet](../Docs/residual_block.png)\n",
    "### bottleneck block \n",
    "![ResNet](../Docs/bottleneck_block.png)\n",
    "### Architecture for ImageNet\n",
    "![ResNet](../Docs/resnet_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "\n",
    "\n",
    "        # main pass\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 =  nn.ReLU(inplace=True)\n",
    "\n",
    "        # shortcut pass\n",
    "        if stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # add main-pass and shortcut-pass  \n",
    "        out += identity\n",
    "\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten layer\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_block\n",
    "def resnet_block(in_channels, out_channels, stride, num_residuals, first_block=False):\n",
    "\n",
    "    if first_block:\n",
    "        assert out_channels == out_channels\n",
    "\n",
    "    blk = []\n",
    "\n",
    "    # first Residual execute down-sample\n",
    "    blk.append(Residual(in_channels, out_channels, stride=stride))\n",
    "\n",
    "    # others Residual do not down-sample\n",
    "    for  _ in range(1, num_residuals):\n",
    "        blk.append(Residual(out_channels, out_channels, stride=1))\n",
    "\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## renet_18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base modules\n",
    "model = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residule module\n",
    "model.add_module(\"resnet_block1\", resnet_block(64, 64, 1, 2, first_block=True))\n",
    "model.add_module(\"resnet_block2\", resnet_block(64, 128, 2, 2))\n",
    "model.add_module(\"resnet_block3\", resnet_block(128, 256, 2, 2))\n",
    "model.add_module(\"resnet_block4\", resnet_block(256, 512, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classfify module\n",
    "model.add_module('global_avgpool', nn.AdaptiveAvgPool2d((1, 1)))\n",
    "model.add_module('fc', nn.Sequential(Flatten(), nn.Linear(512, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 64, 112, 112])\n",
      "1 torch.Size([10, 64, 112, 112])\n",
      "2 torch.Size([10, 64, 112, 112])\n",
      "3 torch.Size([10, 64, 56, 56])\n",
      "resnet_block1 torch.Size([10, 64, 56, 56])\n",
      "resnet_block2 torch.Size([10, 128, 28, 28])\n",
      "resnet_block3 torch.Size([10, 256, 14, 14])\n",
      "resnet_block4 torch.Size([10, 512, 7, 7])\n",
      "global_avgpool torch.Size([10, 512, 1, 1])\n",
      "fc torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10, 1, 224, 224)\n",
    "for name, layer in model.named_children():\n",
    "    x = layer(x)\n",
    "    print(name, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, device=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # convert to eval(model)\n",
    "\n",
    "    if device is None and isinstance(model, torch.nn.Module):\n",
    "        # if device is None, use the net device\n",
    "        device = list(model.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)  # load data to device\n",
    "            acc_sum += (model(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += x.shape[0]\n",
    "\n",
    "    print('Eval epoch {} => acc {:.4f}'.format(epoch, acc_sum / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss, optimizer, epoch, device=None):\n",
    "    \"\"\"\n",
    "    convert train model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_acc, train_loss, num_samples = 0, 0.0, 0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_y = model(x)\n",
    "        l = loss(pred_y, y)\n",
    "        # grad clearing\n",
    "        optimizer.zero_grad()\n",
    "        # computer grad\n",
    "        l.backward()\n",
    "        # update grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += l.cpu().item()\n",
    "        train_acc += (pred_y.argmax(dim=1) == y).float().sum().cpu().item()\n",
    "        \n",
    "        num_samples += x.shape[0]\n",
    "        num_batch += 1\n",
    "        \n",
    "    print('Train epoch {} => loss {:.4f}, acc {:.4f}'.\n",
    "          format(epoch, train_loss / num_batch, train_acc / num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 0.4289, acc 0.8433\n",
      "Eval epoch 1 => acc 0.8667\n",
      "Train epoch 2 => loss 0.2657, acc 0.9024\n",
      "Eval epoch 2 => acc 0.8947\n",
      "Train epoch 3 => loss 0.2255, acc 0.9163\n",
      "Eval epoch 3 => acc 0.9141\n",
      "Train epoch 4 => loss 0.1918, acc 0.9274\n",
      "Eval epoch 4 => acc 0.9098\n",
      "Train epoch 5 => loss 0.1763, acc 0.9349\n",
      "Eval epoch 5 => acc 0.9006\n",
      "Train epoch 6 => loss 0.1530, acc 0.9431\n",
      "Eval epoch 6 => acc 0.9171\n",
      "Train epoch 7 => loss 0.1379, acc 0.9489\n",
      "Eval epoch 7 => acc 0.9171\n",
      "Train epoch 8 => loss 0.1160, acc 0.9566\n",
      "Eval epoch 8 => acc 0.9145\n",
      "Train epoch 9 => loss 0.1036, acc 0.9614\n",
      "Eval epoch 9 => acc 0.9112\n",
      "Train epoch 10 => loss 0.0823, acc 0.9695\n",
      "Eval epoch 10 => acc 0.9181\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "lr, gamma = 0.001, 0.9\n",
    "model = model.to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size, size=(96, 96))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
