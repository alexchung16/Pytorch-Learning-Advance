{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 残差网络\n",
    "\n",
    "构建深度神经网络时会遇到两个主要问题：梯度爆炸/消散和网络退化问题。\n",
    "* 梯度爆炸/消散，是由于随着层数的增加，在网络反向传播过程中梯度会随着级乘运算变得特别大或特别小，即梯度变得不稳定。可以通过BatchNrom技术解决。\n",
    "* 网络退化，通常认为，随着网络深度的增加，网络的性能会获得相应的提升。**但是，我们发现当网络增加到一定程度后继续增加，网络的性能会变得越來越差，直接体现为训练集上的准确率会下降。** 我们假设通过简单地对叠方式得到很深的网络，网络内部的特征在某一层（浅层）已经达到最佳的性能，此时该网络的浅层形式的解空间是深层模型解空间的子空间。也就是说，如果我们能够将达到最佳性能的层之后的层训练成恒等映射，且深层网络可能得出更优的解来拟合训练集，因此深层网络能够更容易地降低训练误差。**但是，由于网络退化的问题，这一假设并不成立。**\n",
    "* 通过分析，我们退而求其次，在已知深层网络存在退化的的情况下，寻求方法解决深层网络的退化问题，使得网络至少实现深层网络和浅层网络具有一样的性能。即让深层网络后面的部分即使不提升网络性能的情况下，至少能够实现恒等映射的作用，使得网络的性能不会随着深度的增加而出现退化。Residual模块被提出来解决这一问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "### residual block\n",
    "![ResNet](../Docs/residual_block.png)\n",
    "### bottleneck block \n",
    "![ResNet](../Docs/bottleneck_block.png)\n",
    "### Architecture for ImageNet\n",
    "![ResNet](../Docs/resnet_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
