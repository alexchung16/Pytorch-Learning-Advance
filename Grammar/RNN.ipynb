{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于RNN 的字符生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](../Docs/RNN-Core.png) \n",
    "\n",
    "<center>http://dprogrammer.org/rnn-lstm-gru</center>\n",
    "\n",
    "\n",
    "* 更新隐藏状态\n",
    "$$\\boldsymbol{H}t = \\phi(\\boldsymbol{X}t \\boldsymbol{W}{xh} + \\boldsymbol{H}{t-1} \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h).$$\n",
    "\n",
    "* 计算输出\n",
    "$$\\boldsymbol{O}_t = \\boldsymbol{H}t \\boldsymbol{W}{hq} + \\boldsymbol{b}_q.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 困惑度\n",
    "通常使用困惑度（complexity）来评价语言模型的好坏。\n",
    "\n",
    "**困惑度是对交叉损失函数做指数运算后得到的值。**\n",
    "\n",
    "**任何一个有效模型的困惑度必须小于类别个数。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import zipfile\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import utils as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(object):\n",
    "    def __init__(self, text=None, max_vocab=None, filename=None):\n",
    "        if filename is not None:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.vocab = pickle.load(f)\n",
    "        else:\n",
    "            corpus_chars = set(text)\n",
    "            # max_vocab_process\n",
    "            vocab_count = {}\n",
    "            for word in corpus_chars:\n",
    "                if vocab_count.get(word) is None:\n",
    "                    vocab_count[word] = 1\n",
    "                else:\n",
    "                    vocab_count[word] += 1\n",
    "\n",
    "            vocab_count_list = []\n",
    "            for word in vocab_count:\n",
    "                vocab_count_list.append((word, vocab_count[word]))\n",
    "            # sort count with number\n",
    "            vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            if max_vocab is not None and len(vocab_count_list) > max_vocab:\n",
    "                vocab_count_list = vocab_count_list[:max_vocab]\n",
    "            vocab = [x[0] for x in vocab_count_list]\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.char_to_index = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.index_to_char = dict(enumerate(self.vocab))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        if word in self.char_to_index:\n",
    "            return self.char_to_index[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def index_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.char_to_index[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_array(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_index(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def array_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.index_to_word(index))\n",
    "        return \"\".join(words)\n",
    "\n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_random(corpus_indices, seq_length, time_steps, device=None):\n",
    "    num_samples = (len(corpus_indices) - 1) // time_steps\n",
    "    epoch_size = num_samples // seq_length\n",
    "\n",
    "    # shuffle sample\n",
    "    samples_indices = list(range(num_samples))\n",
    "    random.shuffle(samples_indices)\n",
    "\n",
    "    # generator data\n",
    "    for i in range(epoch_size):\n",
    "        i = i * seq_length\n",
    "\n",
    "        batch_incices = samples_indices[i: i + seq_length]\n",
    "        x = [corpus_indices[indices: indices + time_steps] for indices in batch_incices]\n",
    "        y = [corpus_indices[indices + 1: indices + time_steps + 1] for indices in batch_incices]\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32).view(seq_length, time_steps)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(seq_length, time_steps)\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_consecutive(corpus_indices, seq_length, time_steps):\n",
    "\n",
    "    data_len = len(corpus_indices)\n",
    "\n",
    "    seq_size = data_len // seq_length\n",
    "\n",
    "    # resize to => (seq_length, seq_size)\n",
    "    corpus_indices = np.array(corpus_indices[: seq_size * seq_length], dtype=np.float).reshape((seq_length, -1))\n",
    "\n",
    "    epoch_size = (seq_size - 1) // time_steps\n",
    "\n",
    "    # generator data\n",
    "    np.random.shuffle(corpus_indices)\n",
    "\n",
    "    # convert to torch tensor\n",
    "    torch_indices = torch.tensor(corpus_indices, dtype=torch.float32).view(seq_length, seq_size)\n",
    "    for i in range(epoch_size):\n",
    "        i = i * time_steps\n",
    "        x = torch_indices[:, i: i + time_steps]\n",
    "        y = torch_indices[:, i + 1: i + time_steps + 1]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../Datasets/jaychou_lyrics/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')  # corpus\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\t', ' ')\n",
    "corpus_chars = corpus_chars[: 20000]\n",
    "\n",
    "tokenizer = TextTokenizer(text=corpus_chars, max_vocab=None)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "char_to_index = tokenizer.char_to_index\n",
    "index_to_char = tokenizer.index_to_char\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "corpus_indices = tokenizer.text_to_array(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1448])\n"
     ]
    }
   ],
   "source": [
    "def onehot(X, n_class):\n",
    "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n",
    "\n",
    "    return [F.one_hot(X[:, i].to(torch.int64), n_class).to(dtype=torch.float32, device=device) for i in range(X.shape[1])]\n",
    "\n",
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # hidden params\n",
    "    w_xh = _one((num_inputs, num_hiddens))\n",
    "    w_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device), requires_grad=True)\n",
    "    # outputs params\n",
    "    w_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device), requires_grad=True)\n",
    "    return nn.ParameterList([w_xh, w_hh, b_h, w_hq, b_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(seq_length, num_hiddens, device):\n",
    "    return (torch.zeros((seq_length, num_hiddens), device=device),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    \"\"\"\n",
    "    ;inputs (seq_length, vocab_size)\n",
    "    ;state (seq_length, vocab_size)\n",
    "    \"\"\"\n",
    "    w_xh, w_hh, b_h, w_hq, b_q = params\n",
    "    h, = state\n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        h = torch.tanh(torch.matmul(x, w_xh) + torch.matmul(h, w_hh) + b_h)\n",
    "        y = torch.matmul(h, w_hq) + b_q\n",
    "        outputs.append(y)\n",
    "    return outputs, (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1448])\n",
      "5 torch.Size([2, 1448]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(10).view(2, 5)\n",
    "state = init_rnn_state(X.shape[0], num_hiddens, device)\n",
    "inputs = onehot(X.to(device), vocab_size)\n",
    "print(inputs[0].size())\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens, device)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分开驳伦师狼跑境否纵偎衷\n"
     ]
    }
   ],
   "source": [
    "p = predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "                    device, index_to_char, char_to_index)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, seq_length, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(seq_length, num_hiddens, device)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, seq_length, num_steps, device)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(seq_length, num_hiddens, device)\n",
    "            else:\n",
    "                #\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "\n",
    "            inputs = onehot(X, vocab_size)\n",
    "            # outputs有num_steps个形状为(seq_length, vocab_size)的矩阵\n",
    "            (outputs, state) = rnn(inputs, state, params)\n",
    "            # 拼接之后形状为(num_steps * seq_length, vocab_size)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            # Y的形状是(seq_length, num_steps)，转置后再变成长度为\n",
    "            # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "            # 使用交叉熵损失计算平均分类误差\n",
    "            l = loss(outputs, y.long())\n",
    "\n",
    "            # 梯度清0\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                                        num_hiddens, vocab_size, device, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, seq_length, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 49.107354, time 0.23 sec\n",
      " - 分开 我想要你开经天 我想要你已经天 我想要你已经天 我想要你已经天 我想要你已经天 我想要你已经天 我\n",
      " - 不分开亚 我想要你已经天人 不要躲 我只我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 \n",
      "epoch 100, perplexity 7.486892, time 0.23 sec\n",
      " - 分开 有什么 一步两步三步四步 连成线背著风默默许 泪被泡 一颗两步三颗四颗 连成线背著背默默许 泪被泡\n",
      " - 不分开  我想能你是碎没人  爷人了天 我的爱空 你的完空 我的是受 你想想要 我的手受 你的爱空 你想想\n",
      "epoch 150, perplexity 3.258661, time 0.22 sec\n",
      " - 分开 有谁时 苦直她 说沉的公式依 不想就 让我有睛都是我不家 想要这种烟我的那望就怎么小容 我们在伊斯\n",
      " - 不分开吗 我想能你心碎没人帮你擦眼泪 别管那身边拥要我们感到  走在我们 我不要我想你 不能痛的我 你牵着\n",
      "epoch 200, perplexity 2.394487, time 0.23 sec\n",
      " - 分开 有谁心 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星是否\n",
      " - 不分开扫 一后在人落 又街是 你在店 我就店事静的黑样  一天两只孤颗 却满 在又谓常我知不到  我连能 \n",
      "epoch 250, perplexity 2.147796, time 0.23 sec\n",
      " - 分开 只杰有我 我面不用逃活 我该你会会小着 没身个我已分开都是就时的泪 从雨高人到我的微 小成无望 我\n",
      " - 不分开时 我想你爸 你单爱可 难去下容 你在己容 在你不觉 不知再性 在一种一 周是一种是你 快物线的风 \n"
     ]
    }
   ],
   "source": [
    "# random sample\n",
    "random_sample = True\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, index_to_char,\n",
    "                      char_to_index, random_sample, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, seq_length, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 36.253188, time 0.23 sec\n",
      " - 分开 我不想再你 我不着你想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我\n",
      " - 不分开你 别不是我满 你的可界 你想是我 我已是你 我想要你 我已不起 我已不起 你已了这 我已了这样我 \n",
      "epoch 100, perplexity 5.437793, time 0.22 sec\n",
      " - 分开 我留好陪你 他后的泪丽 你在前里忆 别乡在战落 决里事不准 别蝪一种奇 没在什么走 三里在斗落 这\n",
      " - 不分开 我在等这生活 我看好好生活 开静不到我不要 他小你 一场两步三步四颗望著天 看星星 一颗两颗三颗四\n",
      "epoch 150, perplexity 2.707171, time 0.22 sec\n",
      " - 分开 小只会 有你 在东里的旧道 而檐想是我的侧膀 说风的风 干风的钥匙 银后前壶 你已是我 你你让天球\n",
      " - 不分开整离 我们着你开 没人 你不么 不能走不好 有在没人气的 我该着你生了 如果歌 后手走 一步两步三步\n",
      "epoch 200, perplexity 2.314698, time 0.24 sec\n",
      " - 分开 我留放陪你 强忍着我的证据 可晶莹的我滴否的嘴 是等中雨后 这样没有天 这乡就不准我再提 我太着陪\n",
      " - 不分开 家风我有表后是一场 用化在一次遇见的你 J 我的之里幽的黑道 带铁在天 在水的背后 银制茶壶 装蟑\n",
      "epoch 250, perplexity 2.414160, time 0.22 sec\n",
      " - 分开 我留远陪我弃 你说啊赢不了 你永远赢不了 我永远做不到 你永远赢不了 我永远做不到 你永远赢不了 \n",
      " - 不分开整家 他静像一直两 在铁时种天排 渴望不起 小使用一个秋 后知后觉 又过了一个秋 后知后觉 又过了一\n"
     ]
    }
   ],
   "source": [
    "# consecutive \n",
    "random_sample = False\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, index_to_char,\n",
    "                      char_to_index, random_sample, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, seq_length, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
