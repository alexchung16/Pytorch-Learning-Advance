{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批归一化（LeNet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import utils as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, size=None, num_workers=4):\n",
    "    \n",
    "    # dataset process\n",
    "    trans = []\n",
    "    if size:\n",
    "        trans.append(torchvision.transforms.Resize(size=size))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    \n",
    "    # load \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=True, download=True,\n",
    "                                                    transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=False, download=True,\n",
    "                                                   transform=transform)\n",
    "    # generate\n",
    "    train_generator = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_generator = data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# resize to 28 * 28\n",
    "train_generator, test_generator = load_dataset(batch_size=256, size=(32, 32))\n",
    "for x, y in train_generator:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        # 1,32,32\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 6,28 ,28\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2) # 6,14,14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 16,10,10\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2) # 16,5,5\n",
    "        \n",
    "        # flatten 16*5*5\n",
    "        \n",
    "        # Linear: in_features, out_features, bias=True\n",
    "        # fc1 \n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        # fc2\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        # fc3\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, device=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # convert to eval(model)\n",
    "\n",
    "    if device is None and isinstance(model, torch.nn.Module):\n",
    "        # if device is None, use the net device\n",
    "        device = list(model.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)  # load data to device\n",
    "            acc_sum += (model(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += x.shape[0]\n",
    "\n",
    "    print('Eval epoch {} => acc {:.4f}'.format(epoch, acc_sum / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss, optimizer, epoch, device=None):\n",
    "    \"\"\"\n",
    "    convert train model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_acc, train_loss, num_samples = 0, 0.0, 0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_y = model(x)\n",
    "        l = loss(pred_y, y)\n",
    "        # grad clearing\n",
    "        optimizer.zero_grad()\n",
    "        # computer grad\n",
    "        l.backward()\n",
    "        # update grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += l.cpu().item()\n",
    "        train_acc += (pred_y.argmax(dim=1) == y).float().sum().cpu().item()\n",
    "        \n",
    "        num_samples += x.shape[0]\n",
    "        num_batch += 1\n",
    "        \n",
    "    print('Train epoch {} => loss {:.4f}, acc {:.4f}'.\n",
    "          format(epoch, train_loss / num_batch, train_acc / num_samples))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDM  优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 2.3129, acc 0.0995\n",
      "Eval epoch 1 => acc 0.1000\n",
      "Train epoch 2 => loss 2.3070, acc 0.1006\n",
      "Eval epoch 2 => acc 0.1000\n",
      "Train epoch 3 => loss 1.9592, acc 0.2214\n",
      "Eval epoch 3 => acc 0.5751\n",
      "Train epoch 4 => loss 0.7987, acc 0.6879\n",
      "Eval epoch 4 => acc 0.7504\n",
      "Train epoch 5 => loss 0.5677, acc 0.7793\n",
      "Eval epoch 5 => acc 0.7904\n",
      "Train epoch 6 => loss 0.4811, acc 0.8161\n",
      "Eval epoch 6 => acc 0.8082\n",
      "Train epoch 7 => loss 0.4345, acc 0.8392\n",
      "Eval epoch 7 => acc 0.8357\n",
      "Train epoch 8 => loss 0.3834, acc 0.8601\n",
      "Eval epoch 8 => acc 0.8508\n",
      "Train epoch 9 => loss 0.3490, acc 0.8722\n",
      "Eval epoch 9 => acc 0.8614\n",
      "Train epoch 10 => loss 0.3303, acc 0.8795\n",
      "Eval epoch 10 => acc 0.8751\n",
      "Train epoch 11 => loss 0.3172, acc 0.8844\n",
      "Eval epoch 11 => acc 0.8740\n",
      "Train epoch 12 => loss 0.2973, acc 0.8907\n",
      "Eval epoch 12 => acc 0.8739\n",
      "Train epoch 13 => loss 0.2856, acc 0.8952\n",
      "Eval epoch 13 => acc 0.8904\n",
      "Train epoch 14 => loss 0.2717, acc 0.9000\n",
      "Eval epoch 14 => acc 0.8762\n",
      "Train epoch 15 => loss 0.2708, acc 0.9004\n",
      "Eval epoch 15 => acc 0.8858\n",
      "Train epoch 16 => loss 0.2596, acc 0.9047\n",
      "Eval epoch 16 => acc 0.8862\n",
      "Train epoch 17 => loss 0.2564, acc 0.9062\n",
      "Eval epoch 17 => acc 0.8801\n",
      "Train epoch 18 => loss 0.2467, acc 0.9100\n",
      "Eval epoch 18 => acc 0.8856\n",
      "Train epoch 19 => loss 0.2419, acc 0.9112\n",
      "Eval epoch 19 => acc 0.8933\n",
      "Train epoch 20 => loss 0.2348, acc 0.9146\n",
      "Eval epoch 20 => acc 0.8838\n"
     ]
    }
   ],
   "source": [
    "# SGDM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.5, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)\n",
    "    # print('epoch {} optimizer learning rate {}'.format(epoch+1, optimizer.param_groups[0]['lr'][0]))\n",
    "    # print('epoch {} scheduler learning rate {}'.format(epoch+1, scheduler.get_lr()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 1.3642, acc 0.4644\n",
      "Eval epoch 1 => acc 0.7413\n",
      "Train epoch 2 => loss 0.5565, acc 0.7845\n",
      "Eval epoch 2 => acc 0.8051\n",
      "Train epoch 3 => loss 0.4548, acc 0.8258\n",
      "Eval epoch 3 => acc 0.8195\n",
      "Train epoch 4 => loss 0.4007, acc 0.8482\n",
      "Eval epoch 4 => acc 0.8399\n",
      "Train epoch 5 => loss 0.3698, acc 0.8603\n",
      "Eval epoch 5 => acc 0.8590\n",
      "Train epoch 6 => loss 0.3441, acc 0.8699\n",
      "Eval epoch 6 => acc 0.8666\n",
      "Train epoch 7 => loss 0.3269, acc 0.8769\n",
      "Eval epoch 7 => acc 0.8681\n",
      "Train epoch 8 => loss 0.3109, acc 0.8829\n",
      "Eval epoch 8 => acc 0.8703\n",
      "Train epoch 9 => loss 0.3055, acc 0.8832\n",
      "Eval epoch 9 => acc 0.8668\n",
      "Train epoch 10 => loss 0.2949, acc 0.8880\n",
      "Eval epoch 10 => acc 0.8791\n",
      "Train epoch 11 => loss 0.2852, acc 0.8912\n",
      "Eval epoch 11 => acc 0.8792\n",
      "Train epoch 12 => loss 0.2734, acc 0.8962\n",
      "Eval epoch 12 => acc 0.8847\n",
      "Train epoch 13 => loss 0.2692, acc 0.8983\n",
      "Eval epoch 13 => acc 0.8787\n",
      "Train epoch 14 => loss 0.2597, acc 0.9012\n",
      "Eval epoch 14 => acc 0.8844\n",
      "Train epoch 15 => loss 0.2556, acc 0.9019\n",
      "Eval epoch 15 => acc 0.8866\n",
      "Train epoch 16 => loss 0.2494, acc 0.9051\n",
      "Eval epoch 16 => acc 0.8865\n",
      "Train epoch 17 => loss 0.2451, acc 0.9060\n",
      "Eval epoch 17 => acc 0.8859\n",
      "Train epoch 18 => loss 0.2397, acc 0.9084\n",
      "Eval epoch 18 => acc 0.8822\n",
      "Train epoch 19 => loss 0.2339, acc 0.9114\n",
      "Eval epoch 19 => acc 0.8873\n",
      "Train epoch 20 => loss 0.2287, acc 0.9132\n",
      "Eval epoch 20 => acc 0.8857\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.01, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDM 与 Adam 训练过程的区别：\n",
    "* SGDM: 训练初期，模型性能较差，误差较大，准确率很低; 训练后期，模型性能快速提升，最终理想的效果。\n",
    "* Adam: 训练初期，模型很快收敛，误差较小，准确率较高; 训练后期，模型性能提升较慢，最终达到理想的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批归一化\n",
    "\n",
    "批归一化（batch normalization)是为了解决深度模型训练过程中内部协变量转移（internal covariate shift)的问题。\n",
    "\n",
    "在训练过程中利用小批量的样本的均值和标准差执行批归一化，不断调整神经网络的中间输出，使得每一个神经层的输入分布在训练过程中保持一致，从而使整个神经网络在各层的中间输出的数值更稳定，较深的神经网络的训练变得容易。\n",
    "\n",
    "### 全连接层\n",
    "\n",
    "对于全连接层，批量归一化层通常置于全连接层中的仿射变换和激活函数之间，**使用整个仿射变换的输出做批归一化**\n",
    "\n",
    "### 卷积层\n",
    "\n",
    "对于卷积层， 批量归一化发生在卷积计算之后， 应用于激活函数之前。如果卷积计算十余处多个通道，需要对这些通道的输出分别做批归一化，其每个通道都拥有独立的拉伸和便宜系数。\n",
    "\n",
    "### 预测时的批归一化\n",
    "\n",
    "通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们对隐藏单元z值进行调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "def batch_norm(x, gamma, beta, moving_mean, moving_var, eps, momentum, training=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # judge is trainging\n",
    "    if not training:\n",
    "        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(x.shape) in [2, 4] # only support full-connect or convolution\n",
    "        # full-connect layer\n",
    "        if len(x.shape) == 2: \n",
    "            # compuer mean and variance on batch dimension\n",
    "            mean = x.mean(dim=0) \n",
    "            var = ((x - mean) ** 2).mean(dim=0)\n",
    "        # convelution layer\n",
    "        else:\n",
    "            # computer mean and variance on channel dimension\n",
    "            mean = x.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((x-mean)**2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # training mode use mean and variance of   current batch  \n",
    "        x_hat = (x - mean) / torch.sqrt(var + eps)\n",
    "        \n",
    "        # update moving_mean and moving_var\n",
    "        moving_mean = momentum * moving_mean + (1 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1 - momentum) * var\n",
    "        \n",
    "    # scale transform \n",
    "    y = gamma * x_hat + beta\n",
    "    \n",
    "    return y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BatchNorm layer\n",
    "class BatchNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_feature, num_dim, eps=1e-5, momentum=0.1):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        assert num_dim in [2, 4]\n",
    "        if num_dim == 2:\n",
    "            shape = [1, num_feature]\n",
    "        else:\n",
    "            shape = [1, num_feature, 1, 1]\n",
    "        \n",
    "        # involve in grad iterator\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        \n",
    "        # not involve in grad iterator\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure x. moving_mean, moving_var in euqual device \n",
    "        if self.moving_mean.device != x.device:\n",
    "            self.moving_mean = self.moving_mean.to(x.device)\n",
    "            self.moving_var = self.moving_var.to(x.device)\n",
    "        # note self.traing=True when model.train() else self.traing=True when model.eval()\n",
    "        y, self.moving_mean, self.moving_var = batch_norm(x, self.gamma, self.beta, self.moving_mean, \n",
    "                                                          self.moving_var, self.eps, self.momentum, \n",
    "                                                          training=self.training)\n",
    "        \n",
    "        return y \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        # 1,32,32\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 6,28 ,28\n",
    "        self.batch_norm1 = BatchNorm(num_feature=6, num_dim=4)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2) # 6,14,14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 16,10,10\n",
    "        self.batch_norm2 = BatchNorm(num_feature=16, num_dim=4)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2) # 16,5,5\n",
    "        \n",
    "        # flatten 16*5*5\n",
    "        \n",
    "        # Linear: in_features, out_features, bias=True\n",
    "        # fc1 \n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.batch_norm3 = BatchNorm(num_feature=120, num_dim=2)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        # fc2\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.batch_norm4= BatchNorm(num_feature=84, num_dim=2)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        # fc3\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch_norm1): BatchNorm()\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch_norm2): BatchNorm()\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (batch_norm3): BatchNorm()\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (batch_norm4): BatchNorm()\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "batch_norm1.gamma torch.Size([1, 6, 1, 1])\n",
      "batch_norm1.beta torch.Size([1, 6, 1, 1])\n",
      "conv2.weight torch.Size([16, 6, 5, 5])\n",
      "conv2.bias torch.Size([16])\n",
      "batch_norm2.gamma torch.Size([1, 16, 1, 1])\n",
      "batch_norm2.beta torch.Size([1, 16, 1, 1])\n",
      "fc1.weight torch.Size([120, 256])\n",
      "fc1.bias torch.Size([120])\n",
      "batch_norm3.gamma torch.Size([1, 120])\n",
      "batch_norm3.beta torch.Size([1, 120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "batch_norm4.gamma torch.Size([1, 84])\n",
      "batch_norm4.beta torch.Size([1, 84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# show parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 0.5923, acc 0.7830\n",
      "Eval epoch 1 => acc 0.8458\n",
      "Train epoch 2 => loss 0.3863, acc 0.8597\n",
      "Eval epoch 2 => acc 0.8408\n",
      "Train epoch 3 => loss 0.3360, acc 0.8766\n",
      "Eval epoch 3 => acc 0.8784\n",
      "Train epoch 4 => loss 0.3063, acc 0.8865\n",
      "Eval epoch 4 => acc 0.8807\n",
      "Train epoch 5 => loss 0.2887, acc 0.8940\n",
      "Eval epoch 5 => acc 0.8785\n",
      "Train epoch 6 => loss 0.2721, acc 0.8978\n",
      "Eval epoch 6 => acc 0.8850\n",
      "Train epoch 7 => loss 0.2605, acc 0.9042\n",
      "Eval epoch 7 => acc 0.8891\n",
      "Train epoch 8 => loss 0.2484, acc 0.9081\n",
      "Eval epoch 8 => acc 0.8908\n",
      "Train epoch 9 => loss 0.2395, acc 0.9113\n",
      "Eval epoch 9 => acc 0.8910\n",
      "Train epoch 10 => loss 0.2267, acc 0.9154\n",
      "Eval epoch 10 => acc 0.8911\n",
      "Train epoch 11 => loss 0.2198, acc 0.9179\n",
      "Eval epoch 11 => acc 0.8899\n",
      "Train epoch 12 => loss 0.2104, acc 0.9218\n",
      "Eval epoch 12 => acc 0.8968\n",
      "Train epoch 13 => loss 0.2093, acc 0.9221\n",
      "Eval epoch 13 => acc 0.8975\n",
      "Train epoch 14 => loss 0.1995, acc 0.9262\n",
      "Eval epoch 14 => acc 0.8966\n",
      "Train epoch 15 => loss 0.1920, acc 0.9287\n",
      "Eval epoch 15 => acc 0.8955\n",
      "Train epoch 16 => loss 0.1848, acc 0.9316\n",
      "Eval epoch 16 => acc 0.8979\n",
      "Train epoch 17 => loss 0.1774, acc 0.9349\n",
      "Eval epoch 17 => acc 0.8882\n",
      "Train epoch 18 => loss 0.1724, acc 0.9369\n",
      "Eval epoch 18 => acc 0.8998\n",
      "Train epoch 19 => loss 0.1672, acc 0.9378\n",
      "Eval epoch 19 => acc 0.8942\n",
      "Train epoch 20 => loss 0.1582, acc 0.9414\n",
      "Eval epoch 20 => acc 0.9021\n"
     ]
    }
   ],
   "source": [
    "# SGDM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.5, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 0.5298, acc 0.8241\n",
      "Eval epoch 1 => acc 0.8626\n",
      "Train epoch 2 => loss 0.3451, acc 0.8748\n",
      "Eval epoch 2 => acc 0.8650\n",
      "Train epoch 3 => loss 0.3081, acc 0.8882\n",
      "Eval epoch 3 => acc 0.8774\n",
      "Train epoch 4 => loss 0.2779, acc 0.8985\n",
      "Eval epoch 4 => acc 0.8720\n",
      "Train epoch 5 => loss 0.2592, acc 0.9048\n",
      "Eval epoch 5 => acc 0.8777\n",
      "Train epoch 6 => loss 0.2407, acc 0.9105\n",
      "Eval epoch 6 => acc 0.8941\n",
      "Train epoch 7 => loss 0.2302, acc 0.9149\n",
      "Eval epoch 7 => acc 0.8954\n",
      "Train epoch 8 => loss 0.2139, acc 0.9215\n",
      "Eval epoch 8 => acc 0.8955\n",
      "Train epoch 9 => loss 0.2040, acc 0.9231\n",
      "Eval epoch 9 => acc 0.8932\n",
      "Train epoch 10 => loss 0.1945, acc 0.9273\n",
      "Eval epoch 10 => acc 0.8961\n",
      "Train epoch 11 => loss 0.1824, acc 0.9319\n",
      "Eval epoch 11 => acc 0.8954\n",
      "Train epoch 12 => loss 0.1723, acc 0.9355\n",
      "Eval epoch 12 => acc 0.8976\n",
      "Train epoch 13 => loss 0.1678, acc 0.9380\n",
      "Eval epoch 13 => acc 0.8959\n",
      "Train epoch 14 => loss 0.1507, acc 0.9439\n",
      "Eval epoch 14 => acc 0.9026\n",
      "Train epoch 15 => loss 0.1456, acc 0.9458\n",
      "Eval epoch 15 => acc 0.8992\n",
      "Train epoch 16 => loss 0.1366, acc 0.9487\n",
      "Eval epoch 16 => acc 0.9020\n",
      "Train epoch 17 => loss 0.1262, acc 0.9528\n",
      "Eval epoch 17 => acc 0.8978\n",
      "Train epoch 18 => loss 0.1176, acc 0.9561\n",
      "Eval epoch 18 => acc 0.8967\n",
      "Train epoch 19 => loss 0.1121, acc 0.9592\n",
      "Eval epoch 19 => acc 0.8935\n",
      "Train epoch 20 => loss 0.1041, acc 0.9620\n",
      "Eval epoch 20 => acc 0.9003\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.01, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 使用批归一化使得模型能够在前期快速收敛，并使得训练过程更稳定。** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
