{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批归一化（LeNet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import utils as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, size=None, num_workers=4):\n",
    "    \n",
    "    # dataset process\n",
    "    trans = []\n",
    "    if size:\n",
    "        trans.append(torchvision.transforms.Resize(size=size))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    \n",
    "    # load \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=True, download=True,\n",
    "                                                    transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=False, download=True,\n",
    "                                                   transform=transform)\n",
    "    # generate\n",
    "    train_generator = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_generator = data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# resize to 28 * 28\n",
    "train_generator, test_generator = load_dataset(batch_size=256, size=(32, 32))\n",
    "for x, y in train_generator:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model \n",
    "\n",
    "！[LeNet](../Docs/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        # 1,32,32\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 6,28 ,28\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2) # 6,14,14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 16,10,10\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2) # 16,5,5\n",
    "        \n",
    "        # flatten 16*5*5\n",
    "        \n",
    "        # Linear: in_features, out_features, bias=True\n",
    "        # fc1 \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        # fc2\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        # fc3\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, device=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # convert to eval(model)\n",
    "\n",
    "    if device is None and isinstance(model, torch.nn.Module):\n",
    "        # if device is None, use the net device\n",
    "        device = list(model.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)  # load data to device\n",
    "            acc_sum += (model(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += x.shape[0]\n",
    "\n",
    "    print('Eval epoch {} => acc {:.4f}'.format(epoch, acc_sum / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss, optimizer, epoch, device=None):\n",
    "    \"\"\"\n",
    "    convert train model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_acc, train_loss, num_samples = 0, 0.0, 0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_y = model(x)\n",
    "        l = loss(pred_y, y)\n",
    "        # grad clearing\n",
    "        optimizer.zero_grad()\n",
    "        # computer grad\n",
    "        l.backward()\n",
    "        # update grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += l.cpu().item()\n",
    "        train_acc += (pred_y.argmax(dim=1) == y).float().sum().cpu().item()\n",
    "        \n",
    "        num_samples += x.shape[0]\n",
    "        num_batch += 1\n",
    "        \n",
    "    print('Train epoch {} => loss {:.4f}, acc {:.4f}'.\n",
    "          format(epoch, train_loss / num_batch, train_acc / num_samples))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDM  优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 2.3122, acc 0.0999\n",
      "Eval epoch 1 => acc 0.1000\n",
      "Train epoch 2 => loss 2.3074, acc 0.1006\n",
      "Eval epoch 2 => acc 0.1000\n",
      "Train epoch 3 => loss 2.3065, acc 0.1003\n",
      "Eval epoch 3 => acc 0.1000\n",
      "Train epoch 4 => loss 1.6388, acc 0.3524\n",
      "Eval epoch 4 => acc 0.6640\n",
      "Train epoch 5 => loss 0.6833, acc 0.7350\n",
      "Eval epoch 5 => acc 0.7603\n",
      "Train epoch 6 => loss 0.5143, acc 0.8003\n",
      "Eval epoch 6 => acc 0.8110\n",
      "Train epoch 7 => loss 0.4454, acc 0.8349\n",
      "Eval epoch 7 => acc 0.8364\n",
      "Train epoch 8 => loss 0.3929, acc 0.8570\n",
      "Eval epoch 8 => acc 0.8533\n",
      "Train epoch 9 => loss 0.3561, acc 0.8706\n",
      "Eval epoch 9 => acc 0.8696\n",
      "Train epoch 10 => loss 0.3324, acc 0.8785\n",
      "Eval epoch 10 => acc 0.8677\n",
      "Train epoch 11 => loss 0.3167, acc 0.8843\n",
      "Eval epoch 11 => acc 0.8754\n",
      "Train epoch 12 => loss 0.3000, acc 0.8897\n",
      "Eval epoch 12 => acc 0.8767\n",
      "Train epoch 13 => loss 0.2869, acc 0.8948\n",
      "Eval epoch 13 => acc 0.8830\n",
      "Train epoch 14 => loss 0.2757, acc 0.8990\n",
      "Eval epoch 14 => acc 0.8870\n",
      "Train epoch 15 => loss 0.2678, acc 0.9019\n",
      "Eval epoch 15 => acc 0.8932\n",
      "Train epoch 16 => loss 0.2550, acc 0.9065\n",
      "Eval epoch 16 => acc 0.8898\n",
      "Train epoch 17 => loss 0.2480, acc 0.9093\n",
      "Eval epoch 17 => acc 0.8882\n",
      "Train epoch 18 => loss 0.2410, acc 0.9120\n",
      "Eval epoch 18 => acc 0.8970\n",
      "Train epoch 19 => loss 0.2372, acc 0.9128\n",
      "Eval epoch 19 => acc 0.8937\n",
      "Train epoch 20 => loss 0.2290, acc 0.9153\n",
      "Eval epoch 20 => acc 0.9008\n"
     ]
    }
   ],
   "source": [
    "# SGDM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.5, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size, size=(32, 32))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)\n",
    "    # print('epoch {} optimizer learning rate {}'.format(epoch+1, optimizer.param_groups[0]['lr'][0]))\n",
    "    # print('epoch {} scheduler learning rate {}'.format(epoch+1, scheduler.get_lr()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 1.2432, acc 0.5060\n",
      "Eval epoch 1 => acc 0.7483\n",
      "Train epoch 2 => loss 0.5392, acc 0.7904\n",
      "Eval epoch 2 => acc 0.8075\n",
      "Train epoch 3 => loss 0.4460, acc 0.8317\n",
      "Eval epoch 3 => acc 0.8367\n",
      "Train epoch 4 => loss 0.3947, acc 0.8513\n",
      "Eval epoch 4 => acc 0.8566\n",
      "Train epoch 5 => loss 0.3686, acc 0.8612\n",
      "Eval epoch 5 => acc 0.8600\n",
      "Train epoch 6 => loss 0.3367, acc 0.8727\n",
      "Eval epoch 6 => acc 0.8561\n",
      "Train epoch 7 => loss 0.3246, acc 0.8770\n",
      "Eval epoch 7 => acc 0.8714\n",
      "Train epoch 8 => loss 0.3061, acc 0.8836\n",
      "Eval epoch 8 => acc 0.8648\n",
      "Train epoch 9 => loss 0.2992, acc 0.8871\n",
      "Eval epoch 9 => acc 0.8781\n",
      "Train epoch 10 => loss 0.2869, acc 0.8921\n",
      "Eval epoch 10 => acc 0.8780\n",
      "Train epoch 11 => loss 0.2813, acc 0.8928\n",
      "Eval epoch 11 => acc 0.8795\n",
      "Train epoch 12 => loss 0.2659, acc 0.8991\n",
      "Eval epoch 12 => acc 0.8802\n",
      "Train epoch 13 => loss 0.2625, acc 0.9012\n",
      "Eval epoch 13 => acc 0.8856\n",
      "Train epoch 14 => loss 0.2524, acc 0.9030\n",
      "Eval epoch 14 => acc 0.8859\n",
      "Train epoch 15 => loss 0.2454, acc 0.9072\n",
      "Eval epoch 15 => acc 0.8828\n",
      "Train epoch 16 => loss 0.2379, acc 0.9103\n",
      "Eval epoch 16 => acc 0.8922\n",
      "Train epoch 17 => loss 0.2324, acc 0.9117\n",
      "Eval epoch 17 => acc 0.8911\n",
      "Train epoch 18 => loss 0.2261, acc 0.9132\n",
      "Eval epoch 18 => acc 0.8835\n",
      "Train epoch 19 => loss 0.2208, acc 0.9157\n",
      "Eval epoch 19 => acc 0.8862\n",
      "Train epoch 20 => loss 0.2118, acc 0.9203\n",
      "Eval epoch 20 => acc 0.8916\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.01, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size, size=(32, 32))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDM 与 Adam 训练过程的区别：\n",
    "* SGDM: 训练初期，模型性能较差，误差较大，准确率很低; 训练后期，模型性能快速提升，最终理想的效果。\n",
    "* Adam: 训练初期，模型很快收敛，误差较小，准确率较高; 训练后期，模型性能提升较慢，最终达到理想的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批归一化\n",
    "\n",
    "批归一化（batch normalization)是为了解决深度模型训练过程中内部协变量转移（internal covariate shift)的问题。\n",
    "\n",
    "在训练过程中利用小批量的样本的均值和标准差执行批归一化，不断调整神经网络的中间输出，使得每一个神经层的输入分布在训练过程中保持一致，从而使整个神经网络在各层的中间输出的数值更稳定，较深的神经网络的训练变得容易。\n",
    "\n",
    "### 全连接层\n",
    "\n",
    "对于全连接层，批量归一化层通常置于全连接层中的仿射变换和激活函数之间，**使用整个仿射变换的输出做批归一化**\n",
    "\n",
    "### 卷积层\n",
    "\n",
    "对于卷积层， 批量归一化发生在卷积计算之后， 应用于激活函数之前。如果卷积计算十余处多个通道，需要对这些通道的输出分别做批归一化，其每个通道都拥有独立的拉伸和便宜系数。\n",
    "\n",
    "### 预测时的批归一化\n",
    "\n",
    "通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们对隐藏单元z值进行调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "def batch_norm(x, gamma, beta, moving_mean, moving_var, eps, momentum, training=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # judge is trainging\n",
    "    if not training:\n",
    "        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(x.shape) in [2, 4] # only support full-connect or convolution\n",
    "        # full-connect layer\n",
    "        if len(x.shape) == 2: \n",
    "            # compuer mean and variance on batch dimension\n",
    "            mean = x.mean(dim=0) \n",
    "            var = ((x - mean) ** 2).mean(dim=0)\n",
    "        # convelution layer\n",
    "        else:\n",
    "            # computer mean and variance on channel dimension\n",
    "            mean = x.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((x-mean)**2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # training mode use mean and variance of   current batch  \n",
    "        x_hat = (x - mean) / torch.sqrt(var + eps)\n",
    "        \n",
    "        # update moving_mean and moving_var\n",
    "        moving_mean = momentum * moving_mean + (1 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1 - momentum) * var\n",
    "        \n",
    "    # scale transform \n",
    "    y = gamma * x_hat + beta\n",
    "    \n",
    "    return y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BatchNorm layer\n",
    "class BatchNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_feature, num_dim, eps=1e-5, momentum=0.1):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        assert num_dim in [2, 4]\n",
    "        if num_dim == 2:\n",
    "            shape = [1, num_feature]\n",
    "        else:\n",
    "            shape = [1, num_feature, 1, 1]\n",
    "        \n",
    "        # involve in grad iterator\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        \n",
    "        # not involve in grad iterator\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure x. moving_mean, moving_var in euqual device \n",
    "        if self.moving_mean.device != x.device:\n",
    "            self.moving_mean = self.moving_mean.to(x.device)\n",
    "            self.moving_var = self.moving_var.to(x.device)\n",
    "        # note self.traing=True when model.train() else self.traing=True when model.eval()\n",
    "        y, self.moving_mean, self.moving_var = batch_norm(x, self.gamma, self.beta, self.moving_mean, \n",
    "                                                          self.moving_var, self.eps, self.momentum, \n",
    "                                                          training=self.training)\n",
    "        \n",
    "        return y \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        # 1,32,32\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 6,28 ,28\n",
    "        self.batch_norm1 = BatchNorm(num_feature=6, num_dim=4)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2) # 6,14,14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 16,10,10\n",
    "        self.batch_norm2 = BatchNorm(num_feature=16, num_dim=4)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2) # 16,5,5\n",
    "        \n",
    "        # flatten 16*5*5\n",
    "        \n",
    "        # Linear: in_features, out_features, bias=True\n",
    "        # fc1 \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.batch_norm3 = BatchNorm(num_feature=120, num_dim=2)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        # fc2\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.batch_norm4= BatchNorm(num_feature=84, num_dim=2)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        # fc3\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch_norm1): BatchNorm()\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch_norm2): BatchNorm()\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (batch_norm3): BatchNorm()\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (batch_norm4): BatchNorm()\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "batch_norm1.gamma torch.Size([1, 6, 1, 1])\n",
      "batch_norm1.beta torch.Size([1, 6, 1, 1])\n",
      "conv2.weight torch.Size([16, 6, 5, 5])\n",
      "conv2.bias torch.Size([16])\n",
      "batch_norm2.gamma torch.Size([1, 16, 1, 1])\n",
      "batch_norm2.beta torch.Size([1, 16, 1, 1])\n",
      "fc1.weight torch.Size([120, 400])\n",
      "fc1.bias torch.Size([120])\n",
      "batch_norm3.gamma torch.Size([1, 120])\n",
      "batch_norm3.beta torch.Size([1, 120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "batch_norm4.gamma torch.Size([1, 84])\n",
      "batch_norm4.beta torch.Size([1, 84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# show parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 0.5412, acc 0.8043\n",
      "Eval epoch 1 => acc 0.8558\n",
      "Train epoch 2 => loss 0.3550, acc 0.8709\n",
      "Eval epoch 2 => acc 0.8721\n",
      "Train epoch 3 => loss 0.3102, acc 0.8871\n",
      "Eval epoch 3 => acc 0.8627\n",
      "Train epoch 4 => loss 0.2807, acc 0.8974\n",
      "Eval epoch 4 => acc 0.8851\n",
      "Train epoch 5 => loss 0.2622, acc 0.9042\n",
      "Eval epoch 5 => acc 0.8929\n",
      "Train epoch 6 => loss 0.2394, acc 0.9124\n",
      "Eval epoch 6 => acc 0.8932\n",
      "Train epoch 7 => loss 0.2261, acc 0.9170\n",
      "Eval epoch 7 => acc 0.8996\n",
      "Train epoch 8 => loss 0.2138, acc 0.9202\n",
      "Eval epoch 8 => acc 0.8920\n",
      "Train epoch 9 => loss 0.2058, acc 0.9232\n",
      "Eval epoch 9 => acc 0.8937\n",
      "Train epoch 10 => loss 0.1942, acc 0.9283\n",
      "Eval epoch 10 => acc 0.9002\n",
      "Train epoch 11 => loss 0.1864, acc 0.9309\n",
      "Eval epoch 11 => acc 0.9010\n",
      "Train epoch 12 => loss 0.1781, acc 0.9344\n",
      "Eval epoch 12 => acc 0.9050\n",
      "Train epoch 13 => loss 0.1716, acc 0.9356\n",
      "Eval epoch 13 => acc 0.9017\n",
      "Train epoch 14 => loss 0.1618, acc 0.9392\n",
      "Eval epoch 14 => acc 0.9041\n",
      "Train epoch 15 => loss 0.1553, acc 0.9422\n",
      "Eval epoch 15 => acc 0.9047\n",
      "Train epoch 16 => loss 0.1459, acc 0.9448\n",
      "Eval epoch 16 => acc 0.9096\n",
      "Train epoch 17 => loss 0.1419, acc 0.9463\n",
      "Eval epoch 17 => acc 0.9072\n",
      "Train epoch 18 => loss 0.1340, acc 0.9501\n",
      "Eval epoch 18 => acc 0.9089\n",
      "Train epoch 19 => loss 0.1278, acc 0.9528\n",
      "Eval epoch 19 => acc 0.9027\n",
      "Train epoch 20 => loss 0.1185, acc 0.9558\n",
      "Eval epoch 20 => acc 0.9052\n"
     ]
    }
   ],
   "source": [
    "# SGDM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.5, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size, size=(32, 32))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 0.5292, acc 0.8264\n",
      "Eval epoch 1 => acc 0.8571\n",
      "Train epoch 2 => loss 0.3369, acc 0.8781\n",
      "Eval epoch 2 => acc 0.8695\n",
      "Train epoch 3 => loss 0.2975, acc 0.8920\n",
      "Eval epoch 3 => acc 0.8883\n",
      "Train epoch 4 => loss 0.2632, acc 0.9047\n",
      "Eval epoch 4 => acc 0.8884\n",
      "Train epoch 5 => loss 0.2484, acc 0.9092\n",
      "Eval epoch 5 => acc 0.8869\n",
      "Train epoch 6 => loss 0.2269, acc 0.9163\n",
      "Eval epoch 6 => acc 0.8944\n",
      "Train epoch 7 => loss 0.2177, acc 0.9198\n",
      "Eval epoch 7 => acc 0.9023\n",
      "Train epoch 8 => loss 0.2030, acc 0.9248\n",
      "Eval epoch 8 => acc 0.8947\n",
      "Train epoch 9 => loss 0.1892, acc 0.9296\n",
      "Eval epoch 9 => acc 0.9028\n",
      "Train epoch 10 => loss 0.1790, acc 0.9347\n",
      "Eval epoch 10 => acc 0.9056\n",
      "Train epoch 11 => loss 0.1690, acc 0.9374\n",
      "Eval epoch 11 => acc 0.9004\n",
      "Train epoch 12 => loss 0.1532, acc 0.9436\n",
      "Eval epoch 12 => acc 0.9017\n",
      "Train epoch 13 => loss 0.1469, acc 0.9460\n",
      "Eval epoch 13 => acc 0.9002\n",
      "Train epoch 14 => loss 0.1323, acc 0.9513\n",
      "Eval epoch 14 => acc 0.8995\n",
      "Train epoch 15 => loss 0.1274, acc 0.9530\n",
      "Eval epoch 15 => acc 0.9010\n",
      "Train epoch 16 => loss 0.1141, acc 0.9583\n",
      "Eval epoch 16 => acc 0.8949\n",
      "Train epoch 17 => loss 0.1045, acc 0.9619\n",
      "Eval epoch 17 => acc 0.8963\n",
      "Train epoch 18 => loss 0.0945, acc 0.9654\n",
      "Eval epoch 18 => acc 0.9006\n",
      "Train epoch 19 => loss 0.0885, acc 0.9681\n",
      "Eval epoch 19 => acc 0.9038\n",
      "Train epoch 20 => loss 0.0805, acc 0.9707\n",
      "Eval epoch 20 => acc 0.8982\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.01, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size, size=(32, 32))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 使用批归一化使得模型能够在前期快速收敛，并使得训练过程更稳定。** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
