{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批归一化（LeNet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import utils as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, size=None, num_workers=4):\n",
    "    \n",
    "    # dataset process\n",
    "    trans = []\n",
    "    if size:\n",
    "        trans.append(torchvision.transforms.Resize(size=size))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    \n",
    "    # load \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=True, download=True,\n",
    "                                                    transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=False, download=True,\n",
    "                                                   transform=transform)\n",
    "    # generate\n",
    "    train_generator = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_generator = data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# resize to 28 * 28\n",
    "train_generator, test_generator = load_dataset(batch_size=256, size=(32, 32))\n",
    "for x, y in train_generator:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        # 1,32,32\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 6,28 ,28\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2) # 6,14,14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 16,10,10\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2) # 16,5,5\n",
    "        \n",
    "        # flatten 16*5*5\n",
    "        \n",
    "        # Linear: in_features, out_features, bias=True\n",
    "        # fc1 \n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        # fc2\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        # fc3\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, device=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # convert to eval(model)\n",
    "\n",
    "    if device is None and isinstance(model, torch.nn.Module):\n",
    "        # if device is None, use the net device\n",
    "        device = list(model.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)  # load data to device\n",
    "            acc_sum += (model(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += x.shape[0]\n",
    "\n",
    "    print('Eval epoch {} => acc {:.4f}'.format(epoch, acc_sum / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss, optimizer, epoch, device=None):\n",
    "    \"\"\"\n",
    "    convert train model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_acc, train_loss, num_samples = 0, 0.0, 0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_y = model(x)\n",
    "        l = loss(pred_y, y)\n",
    "        # grad clearing\n",
    "        optimizer.zero_grad()\n",
    "        # computer grad\n",
    "        l.backward()\n",
    "        # update grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += l.cpu().item()\n",
    "        train_acc += (pred_y.argmax(dim=1) == y).float().sum().cpu().item()\n",
    "        \n",
    "        num_samples += x.shape[0]\n",
    "        num_batch += 1\n",
    "        \n",
    "    print('Train epoch {} => loss {:.4f}, acc {:.4f}'.\n",
    "          format(epoch, train_loss / num_batch, train_acc / num_samples))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "lr, gamma = 1.0, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.8)  # SGDM\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.8) \n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0 => loss 2.3127, acc 0.0989\n",
      "Eval epoch 0 => acc 0.1000\n",
      "Train epoch 1 => loss 2.3069, acc 0.1003\n",
      "Eval epoch 1 => acc 0.1000\n",
      "Train epoch 2 => loss 2.3074, acc 0.1010\n",
      "Eval epoch 2 => acc 0.1000\n",
      "Train epoch 3 => loss 2.3060, acc 0.1003\n",
      "Eval epoch 3 => acc 0.1000\n",
      "Train epoch 4 => loss 1.7413, acc 0.3075\n",
      "Eval epoch 4 => acc 0.6077\n",
      "Train epoch 5 => loss 0.7078, acc 0.7240\n",
      "Eval epoch 5 => acc 0.7648\n",
      "Train epoch 6 => loss 0.5434, acc 0.7889\n",
      "Eval epoch 6 => acc 0.7630\n",
      "Train epoch 7 => loss 0.4637, acc 0.8242\n",
      "Eval epoch 7 => acc 0.8241\n",
      "Train epoch 8 => loss 0.4128, acc 0.8471\n",
      "Eval epoch 8 => acc 0.8376\n",
      "Train epoch 9 => loss 0.3790, acc 0.8596\n",
      "Eval epoch 9 => acc 0.8484\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch, device)\n",
    "    test(model, test_loader, epoch, device=device)\n",
    "    scheduler.step(epoch)\n",
    "    # print('epoch {} optimizer learning rate {}'.format(epoch+1, optimizer.param_groups[0]['lr'][0]))\n",
    "    # print('epoch {} scheduler learning rate {}'.format(epoch+1, scheduler.get_lr()[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
