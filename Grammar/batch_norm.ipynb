{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批归一化（LeNet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import utils as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, size=None, num_workers=4):\n",
    "    \n",
    "    # dataset process\n",
    "    trans = []\n",
    "    if size:\n",
    "        trans.append(torchvision.transforms.Resize(size=size))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    \n",
    "    # load \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=True, download=True,\n",
    "                                                    transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=False, download=True,\n",
    "                                                   transform=transform)\n",
    "    # generate\n",
    "    train_generator = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_generator = data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# resize to 28 * 28\n",
    "train_generator, test_generator = load_dataset(batch_size=256, size=(32, 32))\n",
    "for x, y in train_generator:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        # 1,32,32\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 6,28 ,28\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2) # 6,14,14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 16,10,10\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2) # 16,5,5\n",
    "        \n",
    "        # flatten 16*5*5\n",
    "        \n",
    "        # Linear: in_features, out_features, bias=True\n",
    "        # fc1 \n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        # fc2\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        # fc3\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, device=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # convert to eval(model)\n",
    "\n",
    "    if device is None and isinstance(model, torch.nn.Module):\n",
    "        # if device is None, use the net device\n",
    "        device = list(model.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)  # load data to device\n",
    "            acc_sum += (model(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += x.shape[0]\n",
    "\n",
    "    print('Eval epoch {} => acc {:.4f}'.format(epoch, acc_sum / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss, optimizer, epoch, device=None):\n",
    "    \"\"\"\n",
    "    convert train model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_acc, train_loss, num_samples = 0, 0.0, 0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_y = model(x)\n",
    "        l = loss(pred_y, y)\n",
    "        # grad clearing\n",
    "        optimizer.zero_grad()\n",
    "        # computer grad\n",
    "        l.backward()\n",
    "        # update grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += l.cpu().item()\n",
    "        train_acc += (pred_y.argmax(dim=1) == y).float().sum().cpu().item()\n",
    "        \n",
    "        num_samples += x.shape[0]\n",
    "        num_batch += 1\n",
    "        \n",
    "    print('Train epoch {} => loss {:.4f}, acc {:.4f}'.\n",
    "          format(epoch, train_loss / num_batch, train_acc / num_samples))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDM  优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 2.3121, acc 0.1003\n",
      "Eval epoch 1 => acc 0.1000\n",
      "Train epoch 2 => loss 2.3045, acc 0.1053\n",
      "Eval epoch 2 => acc 0.1807\n",
      "Train epoch 3 => loss 1.2315, acc 0.4931\n",
      "Eval epoch 3 => acc 0.7137\n",
      "Train epoch 4 => loss 0.6405, acc 0.7522\n",
      "Eval epoch 4 => acc 0.7588\n",
      "Train epoch 5 => loss 0.5137, acc 0.8063\n",
      "Eval epoch 5 => acc 0.8139\n",
      "Train epoch 6 => loss 0.4506, acc 0.8298\n",
      "Eval epoch 6 => acc 0.8268\n",
      "Train epoch 7 => loss 0.4103, acc 0.8480\n",
      "Eval epoch 7 => acc 0.8363\n",
      "Train epoch 8 => loss 0.3857, acc 0.8566\n",
      "Eval epoch 8 => acc 0.8463\n",
      "Train epoch 9 => loss 0.3660, acc 0.8629\n",
      "Eval epoch 9 => acc 0.8580\n",
      "Train epoch 10 => loss 0.3443, acc 0.8721\n",
      "Eval epoch 10 => acc 0.8570\n",
      "Train epoch 11 => loss 0.3393, acc 0.8725\n",
      "Eval epoch 11 => acc 0.8700\n",
      "Train epoch 12 => loss 0.3289, acc 0.8763\n",
      "Eval epoch 12 => acc 0.8734\n",
      "Train epoch 13 => loss 0.3247, acc 0.8798\n",
      "Eval epoch 13 => acc 0.8731\n",
      "Train epoch 14 => loss 0.3108, acc 0.8829\n",
      "Eval epoch 14 => acc 0.8728\n",
      "Train epoch 15 => loss 0.3064, acc 0.8855\n",
      "Eval epoch 15 => acc 0.8768\n",
      "Train epoch 16 => loss 0.3005, acc 0.8869\n",
      "Eval epoch 16 => acc 0.8706\n",
      "Train epoch 17 => loss 0.2951, acc 0.8898\n",
      "Eval epoch 17 => acc 0.8801\n",
      "Train epoch 18 => loss 0.2872, acc 0.8922\n",
      "Eval epoch 18 => acc 0.8821\n",
      "Train epoch 19 => loss 0.2831, acc 0.8938\n",
      "Eval epoch 19 => acc 0.8846\n",
      "Train epoch 20 => loss 0.2763, acc 0.8967\n",
      "Eval epoch 20 => acc 0.8845\n"
     ]
    }
   ],
   "source": [
    "# SGDM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.5, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)\n",
    "    # print('epoch {} optimizer learning rate {}'.format(epoch+1, optimizer.param_groups[0]['lr'][0]))\n",
    "    # print('epoch {} scheduler learning rate {}'.format(epoch+1, scheduler.get_lr()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train epoch 1 => loss 1.1344, acc 0.5650\n",
      "Eval epoch 1 => acc 0.7596\n",
      "Train epoch 2 => loss 0.5185, acc 0.7993\n",
      "Eval epoch 2 => acc 0.8056\n",
      "Train epoch 3 => loss 0.4230, acc 0.8414\n",
      "Eval epoch 3 => acc 0.8474\n",
      "Train epoch 4 => loss 0.3606, acc 0.8671\n",
      "Eval epoch 4 => acc 0.8565\n",
      "Train epoch 5 => loss 0.3313, acc 0.8763\n",
      "Eval epoch 5 => acc 0.8635\n",
      "Train epoch 6 => loss 0.3053, acc 0.8860\n",
      "Eval epoch 6 => acc 0.8747\n",
      "Train epoch 7 => loss 0.2922, acc 0.8915\n",
      "Eval epoch 7 => acc 0.8807\n",
      "Train epoch 8 => loss 0.2803, acc 0.8956\n",
      "Eval epoch 8 => acc 0.8800\n",
      "Train epoch 9 => loss 0.2669, acc 0.8996\n",
      "Eval epoch 9 => acc 0.8819\n",
      "Train epoch 10 => loss 0.2548, acc 0.9040\n",
      "Eval epoch 10 => acc 0.8916\n",
      "Train epoch 11 => loss 0.2509, acc 0.9047\n",
      "Eval epoch 11 => acc 0.8795\n",
      "Train epoch 12 => loss 0.2399, acc 0.9097\n",
      "Eval epoch 12 => acc 0.8936\n",
      "Train epoch 13 => loss 0.2337, acc 0.9113\n",
      "Eval epoch 13 => acc 0.8929\n",
      "Train epoch 14 => loss 0.2253, acc 0.9147\n",
      "Eval epoch 14 => acc 0.8922\n",
      "Train epoch 15 => loss 0.2197, acc 0.9173\n",
      "Eval epoch 15 => acc 0.8939\n",
      "Train epoch 16 => loss 0.2108, acc 0.9200\n",
      "Eval epoch 16 => acc 0.8929\n",
      "Train epoch 17 => loss 0.2066, acc 0.9232\n",
      "Eval epoch 17 => acc 0.8924\n",
      "Train epoch 18 => loss 0.2002, acc 0.9241\n",
      "Eval epoch 18 => acc 0.8977\n",
      "Train epoch 19 => loss 0.1941, acc 0.9269\n",
      "Eval epoch 19 => acc 0.8970\n",
      "Train epoch 20 => loss 0.1888, acc 0.9293\n",
      "Eval epoch 20 => acc 0.8965\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  \n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "lr, gamma = 0.01, 0.9\n",
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)  # SGDM\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # Adam\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=gamma)\n",
    "\n",
    "train_loader, test_loader = load_dataset(batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, loss, optimizer, epoch+1, device)\n",
    "    test(model, test_loader, epoch+1, device=device)\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDM 与 Adam 训练过程的区别：\n",
    "* SGDM: 训练初期，模型性能较差，误差较大，准确率很低; 训练后期，模型性能快速提升，最终理想的效果。\n",
    "* Adam: 训练初期，模型很快收敛，误差较小，准确率较高; 训练后期，模型性能提升较慢，最终达到理想的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
