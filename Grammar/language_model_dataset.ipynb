{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型数据集(周杰伦专辑歌词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../Datasets/jaychou_lyrics/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')  # corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_chars))\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus process\n",
    "# replace '\\n' or '\\t' with ' '\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\t', ' ')\n",
    "corpus_chars = corpus_chars[: 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_chars))\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab count\n",
    "def vocab_count(text, max_vocab=None):\n",
    "    \n",
    "    vocab = set(corpus_chars)\n",
    "    vocab_counts = {}\n",
    "\n",
    "    # initial vocab_count\n",
    "    for word in corpus_chars:\n",
    "        if vocab_counts.get(word) is None:\n",
    "            vocab_counts[word] = 1\n",
    "        else:\n",
    "            vocab_counts[word] += 1\n",
    "\n",
    "    vocab_count_list = []\n",
    "    for word, count in vocab_counts.items():\n",
    "\n",
    "        vocab_count_list.append((word, count))\n",
    "\n",
    "    # sort according to word count from large to small\n",
    "    vocab_count_list.sort(key=lambda x: x[1], reverse=True) \n",
    "     \n",
    "    if max_vocab is not None and len(vocab_count_list) > max_vocab:\n",
    "        vocab_count_list = vocab_count_list[:max_vocab]\n",
    "        \n",
    "    vocab = [x[0] for x in vocab_count_list]\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_count(corpus_chars)\n",
    "char_to_index = {c:i for i, c in enumerate(vocab)}\n",
    "index_to_char = dict(enumerate(vocab))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(word):\n",
    "    if word in char_to_index:\n",
    "        return char_to_index[word]\n",
    "    else:\n",
    "        return len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_word(index):\n",
    "    if index == len(vocab):\n",
    "        return '<unk>'\n",
    "    elif index < len(vocab):\n",
    "        return index_to_char[index]\n",
    "    else:\n",
    "        raise Exception('Unknown index!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 17, 7, 79, 950, 591, 0, 8, 17, 111, 3, 208, 14, 679, 680, 32, 0, 8, 17, 111, 3, 681, 165, 6, 5, 67, 0, 681, 165, 6, 679, 680, 58, 0, 2, 135, 15, 135, 15, 135]\n",
      "想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n"
     ]
    }
   ],
   "source": [
    "# convert text to array\n",
    "corpus_indices = [word_to_index(word) for word in corpus_chars]\n",
    "\n",
    "print(corpus_indices[:40])\n",
    "print(''.join([index_to_word(index) for index in corpus_indices[:40]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序数据采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机采样\n",
    "\n",
    "在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。\n",
    "\n",
    "**在训练模型时，每次随机采样前都需要重新初始化隐藏状态。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_random(corpus_indices, seq_length, time_steps, device=None):\n",
    "    num_samples = (len(corpus_indices) - 1) // time_steps\n",
    "    epoch_size = num_samples // seq_length\n",
    "    \n",
    "    # shuffle sample\n",
    "    samples_indices = list(range(num_samples))\n",
    "    random.shuffle(samples_indices)\n",
    "    \n",
    "    # generator data\n",
    "    for i in range(epoch_size):\n",
    "        \n",
    "        i = i * seq_length\n",
    "        \n",
    "        batch_incices = samples_indices[i: i+seq_length]\n",
    "        x = [corpus_indices[indices: indices+time_steps] for indices in batch_incices]\n",
    "        y = [corpus_indices[indices+1: indices+time_steps+1] for indices in batch_incices]\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32).view(seq_length, time_steps)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(seq_length, time_steps)\n",
    "        \n",
    "        yield x, y      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 4., 5.],\n",
      "        [1., 2., 3., 4., 5., 6.]]) \n",
      " tensor([[1., 2., 3., 4., 5., 6.],\n",
      "        [2., 3., 4., 5., 6., 7.]])\n",
      "tensor([[2., 3., 4., 5., 6., 7.],\n",
      "        [3., 4., 5., 6., 7., 8.]]) \n",
      " tensor([[3., 4., 5., 6., 7., 8.],\n",
      "        [4., 5., 6., 7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "demo_seq = list(range(30))\n",
    "for x, y in data_loader_random(demo_seq, seq_length=2, time_steps=6):\n",
    "   print(x , '\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相邻采样\n",
    "\n",
    "**相邻连个随机小批量在原始序列上的位置相毗邻**\n",
    "\n",
    "这时，我们可以用一个小批量的最终时间步的隐藏状态初始化下一个小批量的隐藏状态， 从而使得下一个小批量的输出也取决于当前小批量的输入，如此循环下去。\n",
    "\n",
    "这对实现循环神经网络造成了两方面影响：\n",
    "* 一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；\n",
    "* 另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。\n",
    "\n",
    "** 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_consecutive(corpus_indices, seq_length, time_steps):\n",
    "\n",
    "    data_len = len(corpus_indices)\n",
    "\n",
    "    seq_size = data_len // seq_length\n",
    "\n",
    "    # resize to => (seq_length, seq_size)\n",
    "    corpus_indices = np.array(corpus_indices[: seq_size * seq_length], dtype=np.float).reshape((seq_length, -1))\n",
    "\n",
    "    epoch_size = (seq_size - 1) // time_steps\n",
    "\n",
    "    # generator data\n",
    "    np.random.shuffle(corpus_indices)\n",
    "\n",
    "    # convert to torch tensor\n",
    "    torch_indices = torch.tensor(corpus_indices, dtype=torch.float32).view(seq_length, seq_size)\n",
    "    for i in range(epoch_size):\n",
    "        i = i * time_steps\n",
    "        x = torch_indices[:, i: i + time_steps]\n",
    "        y = torch_indices[:, i + 1: i + time_steps + 1]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [15., 16., 17., 18., 19., 20.]]) \n",
      " tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "        [16., 17., 18., 19., 20., 21.]])\n",
      "tensor([[ 6.,  7.,  8.,  9., 10., 11.],\n",
      "        [21., 22., 23., 24., 25., 26.]]) \n",
      " tensor([[ 7.,  8.,  9., 10., 11., 12.],\n",
      "        [22., 23., 24., 25., 26., 27.]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in data_loader_consecutive(demo_seq, seq_length=2, time_steps=6):\n",
    "   print(x , '\\n', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
