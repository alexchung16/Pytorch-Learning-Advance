{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于GRU的字符生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "\n",
    "门控循环单元（gated recurrent unit，GRU）是一种常用的门控循环神经网络\n",
    "\n",
    "GRU引入了重置门（reset gate）和更新门（update gate）的概念，从而修改了循环神经网络中隐藏状态的计算方式。\n",
    "\n",
    "![GRU](../Docs/GRU-Core.png)\n",
    "<center>http://dprogrammer.org/rnn-lstm-gru</center>\n",
    "\n",
    "* 重置门和更新门计算\n",
    "$$ \\begin{aligned} \\boldsymbol{R}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xr} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hr} + \\boldsymbol{b}_r),\\\\\n",
    "\\boldsymbol{Z}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xz} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hz} + \\boldsymbol{b}_z), \\end{aligned} $$\n",
    "* 候选隐藏状态\n",
    "$$\\tilde{\\boldsymbol{H}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\left(\\boldsymbol{R}_t \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h),$$\n",
    "* 隐藏状态更新\n",
    "$$\\boldsymbol{H}_t = \\boldsymbol{Z}_t \\odot \\boldsymbol{H}_{t-1} + (1 - \\boldsymbol{Z}_t) \\odot \\tilde{\\boldsymbol{H}}_t.$$\n",
    "\n",
    "\n",
    "**重置门有助于捕捉时间序列里短期的依赖关系.**\n",
    "\n",
    "**更新门有助于捕捉时间序列里长期的依赖关系**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import zipfile\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import utils as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(object):\n",
    "    def __init__(self, text=None, max_vocab=None, filename=None):\n",
    "        if filename is not None:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.vocab = pickle.load(f)\n",
    "        else:\n",
    "            corpus_chars = set(text)\n",
    "            # max_vocab_process\n",
    "            vocab_count = {}\n",
    "            for word in corpus_chars:\n",
    "                if vocab_count.get(word) is None:\n",
    "                    vocab_count[word] = 1\n",
    "                else:\n",
    "                    vocab_count[word] += 1\n",
    "\n",
    "            vocab_count_list = []\n",
    "            for word in vocab_count:\n",
    "                vocab_count_list.append((word, vocab_count[word]))\n",
    "            # sort count with number\n",
    "            vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            if max_vocab is not None and len(vocab_count_list) > max_vocab:\n",
    "                vocab_count_list = vocab_count_list[:max_vocab]\n",
    "            vocab = [x[0] for x in vocab_count_list]\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.char_to_index = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.index_to_char = dict(enumerate(self.vocab))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        if word in self.char_to_index:\n",
    "            return self.char_to_index[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def index_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.char_to_index[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_array(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_index(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def array_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.index_to_word(index))\n",
    "        return \"\".join(words)\n",
    "\n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample\n",
    "def data_loader_random(corpus_indices, seq_length, time_steps, device=None):\n",
    "    num_samples = (len(corpus_indices) - 1) // time_steps\n",
    "    epoch_size = num_samples // seq_length\n",
    "\n",
    "    # shuffle sample\n",
    "    samples_indices = list(range(num_samples))\n",
    "    random.shuffle(samples_indices)\n",
    "\n",
    "    # generator data\n",
    "    for i in range(epoch_size):\n",
    "        i = i * seq_length\n",
    "\n",
    "        batch_incices = samples_indices[i: i + seq_length]\n",
    "        x = [corpus_indices[indices: indices + time_steps] for indices in batch_incices]\n",
    "        y = [corpus_indices[indices + 1: indices + time_steps + 1] for indices in batch_incices]\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32).view(seq_length, time_steps)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(seq_length, time_steps)\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consecutive sample\n",
    "def data_loader_consecutive(corpus_indices, seq_length, time_steps):\n",
    "\n",
    "    data_len = len(corpus_indices)\n",
    "\n",
    "    seq_size = data_len // seq_length\n",
    "\n",
    "    # resize to => (seq_length, seq_size)\n",
    "    corpus_indices = np.array(corpus_indices[: seq_size * seq_length], dtype=np.float).reshape((seq_length, -1))\n",
    "\n",
    "    epoch_size = (seq_size - 1) // time_steps\n",
    "\n",
    "    # generator data\n",
    "    np.random.shuffle(corpus_indices)\n",
    "\n",
    "    # convert to torch tensor\n",
    "    torch_indices = torch.tensor(corpus_indices, dtype=torch.float32).view(seq_length, seq_size)\n",
    "    for i in range(epoch_size):\n",
    "        i = i * time_steps\n",
    "        x = torch_indices[:, i: i + time_steps]\n",
    "        y = torch_indices[:, i + 1: i + time_steps + 1]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../Datasets/jaychou_lyrics/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')  # corpus\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\t', ' ')\n",
    "corpus_chars = corpus_chars[: 20000]\n",
    "\n",
    "tokenizer = TextTokenizer(text=corpus_chars, max_vocab=None)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "char_to_index = tokenizer.char_to_index\n",
    "index_to_char = tokenizer.index_to_char\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "corpus_indices = tokenizer.text_to_array(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1448])\n"
     ]
    }
   ],
   "source": [
    "def onehot(x, n_class):\n",
    "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n",
    "\n",
    "    return [F.one_hot(x[:, i].to(torch.int64), n_class).to(dtype=torch.float32, device=device) for i in range(x.shape[1])]\n",
    "\n",
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gru_state(seq_length, num_hiddens, device):\n",
    "    return (torch.zeros((seq_length, num_hiddens), device=device),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "input_size, hidden_size, output_size = vocab_size, 256, vocab_size\n",
    "# initial model params\n",
    "def get_params():\n",
    "    \n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    def _three():\n",
    "        return (_one((input_size, hidden_size)),\n",
    "                _one((hidden_size, hidden_size)),\n",
    "                nn.Parameter(torch.zeros(hidden_size, device=device, dtype=torch.float32),requires_grad=True))\n",
    "    \n",
    "    # hidden params\n",
    "    w_xz, w_hz, b_z = _three() # update gate\n",
    "    w_xr, w_hr, b_r = _three() # reset gate\n",
    "\n",
    "    w_xh, w_hh, b_h = _three()  # candidate hidden state \n",
    "    \n",
    "    # output params\n",
    "    w_hq = _one((hidden_size, output_size))\n",
    "    b_q = nn.Parameter(torch.zeros(output_size, device=device, dtype=torch.float32), requires_grad=True)\n",
    "    \n",
    "    return nn.ParameterList([w_xz, w_hz, b_z, w_xr, w_hr, b_r,  w_xh, w_hh, b_h, w_hq, b_q])\n",
    "\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print('will use', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(inputs, states, params):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs: (time_steps, seq_len, vocab_size)\n",
    "    :param states:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    (h,) = states\n",
    "    w_xz, w_hz, b_z, w_xr, w_hr, b_r,  w_xh, w_hh, b_h, w_hq, b_q = params\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for x in inputs:\n",
    "        z_t = torch.sigmoid(torch.matmul(x, w_xz) + torch.matmul(h, w_hz) + b_z)\n",
    "        r_t = torch.sigmoid(torch.matmul(x, w_xr) + torch.matmul(h, w_hr) + b_r)\n",
    "\n",
    "        h_tilde = torch.tanh(torch.matmul(x, w_xh) + torch.matmul(r_t * h, w_hh) + b_h)\n",
    "        \n",
    "        h = z_t * h + (1 - z_t) * h_tilde\n",
    "        \n",
    "        y = torch.matmul(h, w_hq) + b_q\n",
    "      \n",
    "        outputs.append(y)\n",
    "        \n",
    "    return outputs, (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1448])\n",
      "5 torch.Size([2, 1448]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(10).view(2, 5)\n",
    "state = init_gru_state(X.shape[0], hidden_size, device)\n",
    "inputs = onehot(X.to(device), vocab_size)\n",
    "print(inputs[0].size())\n",
    "params = get_params()\n",
    "outputs, state_new = gru(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gru(prefix, num_chars, gru, params, init_gru_state, \n",
    "                 num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n",
    "    state = init_gru_state(1, num_hiddens, device)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        \n",
    "        # previous last outputs as current input\n",
    "        x = onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n",
    "        # comuper outputs and state\n",
    "        (y, state) = gru(x, state, params)\n",
    "        \n",
    "        # convert output from index to char\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想你汗汗汗汗汗汗汗叫单登\n"
     ]
    }
   ],
   "source": [
    "p = predict_gru('想你', 10, gru, params, init_gru_state, hidden_size, vocab_size,\n",
    "                    device, index_to_char, char_to_index)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_gru(gru, get_params, init_gru_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, seq_length, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = data_loader_random\n",
    "    else:\n",
    "        data_iter_fn = data_loader_consecutive\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # \n",
    "            state = init_gru_state(seq_length, num_hiddens, device)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, seq_length, num_steps)\n",
    "        for X, Y in data_iter:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            if is_random_iter:  \n",
    "                state = init_gru_state(seq_length, num_hiddens, device)\n",
    "            else:\n",
    "                #\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "\n",
    "            inputs = onehot(X, vocab_size)\n",
    "        \n",
    "            (outputs, state) = gru(inputs, state, params)\n",
    "        \n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "        \n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "            # cross entropy\n",
    "            l = loss(outputs, y.long())\n",
    "\n",
    "            # gradient clearing\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            # gradient clearing\n",
    "            grad_clipping(params, clipping_theta, device) \n",
    "            \n",
    "            d2l.sgd(params, lr, 1)\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_gru(prefix, pred_len, gru, params, init_gru_state,\n",
    "                                        num_hiddens, vocab_size, device, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, time_steps, seq_length, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['爱你', '不爱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 1.066066, time 0.52 sec\n",
      " - 爱你 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱\n",
      " - 不爱女人 坏坏的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的\n",
      "epoch 100, perplexity 1.056030, time 0.52 sec\n",
      " - 爱你 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱\n",
      " - 不爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的\n",
      "epoch 150, perplexity 1.055549, time 0.52 sec\n",
      " - 爱你 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱\n",
      " - 不爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的\n",
      "epoch 200, perplexity 1.054593, time 0.52 sec\n",
      " - 爱你 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 如果说怀疑 可以造句如果说分离 能够翻译 如果\n",
      " - 不爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 如果说怀疑 可以造句如果说分离 能够翻译 \n",
      "epoch 250, perplexity 1.053257, time 0.53 sec\n",
      " - 爱你 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱\n",
      " - 不爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的\n"
     ]
    }
   ],
   "source": [
    "# random sample\n",
    "random_sample = True\n",
    "train_and_predict_gru(gru, get_params, init_gru_state, hidden_size,\n",
    "                      vocab_size, device, corpus_indices, index_to_char,\n",
    "                      char_to_index, random_sample, num_epochs, time_steps, lr,\n",
    "                      clipping_theta, seq_length, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 58.862016, time 0.55 sec\n",
      " - 爱你 我不能再想 我不要再想 我不要我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      " - 不爱 我不能再想 我不要再想 我不要我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      "epoch 100, perplexity 4.292827, time 0.53 sec\n",
      " - 爱你走过我好多 好多下耳 就知着我 你知着我不好重 我不能再想 我不要再想 我不 我不 我不要再想你 不\n",
      " - 不爱 你在在角中 有一个人气 家物苦斤的牛肉 让我们 半兽人 的灵魂 单纯 对远古存在的兽 用谦卑的半份\n",
      "epoch 150, perplexity 1.407988, time 0.53 sec\n",
      " - 爱你怎么我要要的一定 从不再说 泪离的泪离 是何检的旧叫 闪通名风豆 爷经的父丽过 帅何了我 全场盯人防\n",
      " - 不爱 在水中中眼离猜 我害怕的想有有样元不了了活 一直是那再已一点糜 他看到过去 试着让你怎么快找 不安\n",
      "epoch 200, perplexity 1.110745, time 0.55 sec\n",
      " - 爱你比过我较多的爸口 河边的风 在小着头发飘动 牵着你的手 一阵莫名感动 我想带你 回我的外婆家 一起看\n",
      " - 不爱 藤水止中带过我 抛入心危梦 登上泰梦的你 得思寄的豆叫画口 无地千年的转生 维地安老斑象的民谣 和\n",
      "epoch 250, perplexity 1.069525, time 0.52 sec\n",
      " - 爱你比何我 说不休 语沉默怎怎么是子汉手 心什么这么简单简详 不懂得酒 如果这一切 真的可以 我想要将我\n",
      " - 不爱 藤非止中在最不到稳 我的等后将被摧毁 他许颓与愿违 累论累　风就平日难平  我们到伊斯坦堡 我才会\n"
     ]
    }
   ],
   "source": [
    "# consecutive sample\n",
    "random_sample = False\n",
    "train_and_predict_gru(gru, get_params, init_gru_state, hidden_size,\n",
    "                      vocab_size, device, corpus_indices, index_to_char,\n",
    "                      char_to_index, random_sample, num_epochs, time_steps, lr,\n",
    "                      clipping_theta, seq_length, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
